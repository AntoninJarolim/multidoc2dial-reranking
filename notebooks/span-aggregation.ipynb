{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-06T14:22:46.290871Z",
     "start_time": "2024-09-06T14:22:46.283282Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:22:46.325882Z",
     "start_time": "2024-09-06T14:22:46.307836Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d7d4debd98f8725c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/xjarol06_firllm/multidoc2dial-reranking'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T14:22:48.690456Z",
     "start_time": "2024-09-06T14:22:46.330836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from visualization_data import InferenceDataProvider, cross_encoder, tokenizer\n",
    "\n",
    "\n"
   ],
   "id": "40b2acb9b4800d0e",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'split_to_tokens' from partially initialized module 'visualization_data' (most likely due to a circular import) (/mnt/data/xjarol06_firllm/multidoc2dial-reranking/visualization_data.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mvisualization_data\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InferenceDataProvider, cross_encoder, tokenizer\n",
      "File \u001B[0;32m/mnt/data/xjarol06_firllm/multidoc2dial-reranking/visualization_data.py:8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01minterpretability\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m attention_rollout, grad_sam, att_cat\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmd2d_dataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m preprocess_examples\n",
      "File \u001B[0;32m/mnt/data/xjarol06_firllm/multidoc2dial-reranking/utils.py:9\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mvisualization_data\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m split_to_tokens\n\u001B[1;32m     11\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmain\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconv_to_torch\u001B[39m(pred, labels):\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'split_to_tokens' from partially initialized module 'visualization_data' (most likely due to a circular import) (/mnt/data/xjarol06_firllm/multidoc2dial-reranking/visualization_data.py)"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from utils import create_grounding_annt_list\n",
    "\n",
    "data_provider = InferenceDataProvider(cross_encoder, tokenizer)\n",
    "nr_annotated_passages = 16\n",
    "nr_annotated_dialogues = 102\n",
    "\n",
    "df_data = []\n",
    "for diag_id in range(nr_annotated_dialogues):\n",
    "\n",
    "    diag_turns, grounded_agent_utterance, nr_show_utterances, rerank_dialog_examples = \\\n",
    "        data_provider.get_dialog_out(diag_id)\n",
    "    inf_out = data_provider.get_dialog_inference_out(diag_id, nr_annotated_passages)\n",
    "\n",
    "    for example in inf_out[\"reranked_examples\"]:\n",
    "        if \"gpt_references\" in example:\n",
    "            passage_tokens, gpt_labels_refs, failed_refs = create_grounding_annt_list(example[\"passage\"],\n",
    "                                                                                      example[\"gpt_references\"],\n",
    "                                                                                      1,\n",
    "                                                                                      return_failed=True)\n",
    "            \n",
    "            if not failed_refs:\n",
    "                passage_list, _ = create_grounding_annt_list(example['x'],\n",
    "                                                             grounded_agent_utterance[\"references\"],\n",
    "                                                             example[\"label\"])\n",
    "                sep_index = [i for i, x in enumerate(passage_list) if x == \"[SEP]\"][0]\n",
    "                passage_list = passage_list[sep_index + 1:]\n",
    "                nr_tokens_trunc_passage = len(passage_list)\n",
    "                \n",
    "                # gpt_labels_refs is for whole passage, but inf_out[\"grad_sam_scores\"] is A[SEP]B\n",
    "                record = {\n",
    "                    \"passage\": example[\"passage\"],\n",
    "                    \"passage_tokens\": passage_tokens,\n",
    "                    \"gpt-labels-refs\": gpt_labels_refs[:nr_tokens_trunc_passage], \n",
    "                    \"passage-tokens\": passage_tokens[:nr_tokens_trunc_passage],\n",
    "                    \"grad-sam-refs\": inf_out[\"grad_sam_scores\"][1:][:-1][sep_index + 1:][:nr_tokens_trunc_passage],\n",
    "                    \"passage_w_dialogue\": example[\"x\"],\n",
    "                }\n",
    "                df_data.append(record)\n",
    "\n",
    "df = pd.DataFrame(df_data)\n",
    "\n",
    "df"
   ],
   "id": "10b0918b0165a181",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Transforming grad-sam refs to the format of gpt_label_refs and opposite\n",
    "\n",
    "# Remove all scores before [SEP]\n",
    "score_colour = score_colour[1:][:-1][:len(passage_list)]\n"
   ],
   "id": "3c1cec86683d5a57",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (md2d-fresh)",
   "language": "python",
   "name": "md2d-fresh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
