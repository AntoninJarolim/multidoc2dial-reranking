train: True
num_epochs: 30
stop_time: 82800
load_model_path: None
save_model_path: ce_lr1e-5_wd1e-1_dr0.1.pt
bert_model_name: FacebookAI/xlm-roberta-base
lr: 1e-05
weight_decay: 0.1
dropout_rate: 0.1
compute_recall_at_k: False
(24.26)------------- Training epoch 0 started -------------
(260.24) Epoch 0 on test dataset: MRR: 0.08; Recalls: tensor([0.0091, 0.0814, 0.1797]) Loss: 24180.26positive-loss: 274.80; negative-loss: 23905.46 
(261.44) Training batch 0/7151 loss: 2.014573335647583
(390.03) Training batch 500/7151 loss: 0.8685157299041748
(518.64) Training batch 1000/7151 loss: 0.9959257245063782
(647.23) Training batch 1500/7151 loss: 0.3430565297603607
(775.81) Training batch 2000/7151 loss: 0.47576668858528137
(906.11) Training batch 2500/7151 loss: 0.9770398139953613
(1034.64) Training batch 3000/7151 loss: 0.5452285408973694
(1163.16) Training batch 3500/7151 loss: 0.30062681436538696
(1291.68) Training batch 4000/7151 loss: 0.8996903896331787
(1420.20) Training batch 4500/7151 loss: 0.5703118443489075
(1548.74) Training batch 5000/7151 loss: 0.47961053252220154
(1677.25) Training batch 5500/7151 loss: 1.1247860193252563
(1805.80) Training batch 6000/7151 loss: 0.3232710361480713
(1934.32) Training batch 6500/7151 loss: 0.8995450735092163
(2062.85) Training batch 7000/7151 loss: 1.0884534120559692
(2101.25) Epoch 0 on training dataset: loss: 36852.31509385677; positive-loss: 33652.17; negative-loss: 3200.15 
(2101.25)------------- Training epoch 1 started -------------
(2333.14) Epoch 1 on test dataset: MRR: 0.44; Recalls: tensor([0.3099, 0.5853, 0.7031]) Loss: 17040.65positive-loss: 275.29; negative-loss: 16765.36 
(2333.40) Training batch 0/7151 loss: 3.678238868713379
(2461.88) Training batch 500/7151 loss: 0.43472081422805786
(2590.37) Training batch 1000/7151 loss: 1.2163052558898926
(2718.86) Training batch 1500/7151 loss: 0.15679755806922913
(2847.36) Training batch 2000/7151 loss: 0.363597571849823
(2976.30) Training batch 2500/7151 loss: 0.958892285823822
(3104.79) Training batch 3000/7151 loss: 0.3460615575313568
(3233.27) Training batch 3500/7151 loss: 0.2006080597639084
(3361.76) Training batch 4000/7151 loss: 0.7941003441810608
(3490.23) Training batch 4500/7151 loss: 0.3976937532424927
(3618.70) Training batch 5000/7151 loss: 0.2658849358558655
(3747.17) Training batch 5500/7151 loss: 0.5907679200172424
(3875.63) Training batch 6000/7151 loss: 0.2877928614616394
(4004.09) Training batch 6500/7151 loss: 0.8026524186134338
(4132.55) Training batch 7000/7151 loss: 0.8098726272583008
(4170.94) Epoch 1 on training dataset: loss: 28920.930877754; positive-loss: 26329.31; negative-loss: 2591.62 
(4170.94)------------- Training epoch 2 started -------------
(4402.79) Epoch 2 on test dataset: MRR: 0.43; Recalls: tensor([0.3047, 0.5762, 0.6816]) Loss: 26544.50positive-loss: 245.38; negative-loss: 26299.12 
(4403.05) Training batch 0/7151 loss: 3.6617984771728516
(4531.52) Training batch 500/7151 loss: 0.25536099076271057
(4659.98) Training batch 1000/7151 loss: 1.288915753364563
(4788.45) Training batch 1500/7151 loss: 0.13281184434890747
(4916.92) Training batch 2000/7151 loss: 0.15968148410320282
(5045.40) Training batch 2500/7151 loss: 0.8616023659706116
(5173.88) Training batch 3000/7151 loss: 0.30062222480773926
(5302.35) Training batch 3500/7151 loss: 0.27470919489860535
(5430.84) Training batch 4000/7151 loss: 0.7435927391052246
(5559.31) Training batch 4500/7151 loss: 0.382442831993103
(5687.79) Training batch 5000/7151 loss: 0.20768649876117706
(5816.26) Training batch 5500/7151 loss: 0.30444443225860596
(5944.73) Training batch 6000/7151 loss: 0.24472688138484955
(6073.21) Training batch 6500/7151 loss: 0.8682751655578613
(6201.68) Training batch 7000/7151 loss: 0.8424360752105713
(6240.07) Epoch 2 on training dataset: loss: 23609.484973451705; positive-loss: 21419.55; negative-loss: 2189.94 
(6240.07)------------- Training epoch 3 started -------------
(6471.29) Epoch 3 on test dataset: MRR: 0.44; Recalls: tensor([0.3112, 0.5710, 0.6960]) Loss: 29788.57positive-loss: 168.93; negative-loss: 29619.65 
(6471.55) Training batch 0/7151 loss: 0.9875019192695618
(6600.03) Training batch 500/7151 loss: 0.0928303599357605
(6728.50) Training batch 1000/7151 loss: 0.7819720506668091
(6856.98) Training batch 1500/7151 loss: 0.09182030707597733
(6985.46) Training batch 2000/7151 loss: 0.1390487253665924
(7113.94) Training batch 2500/7151 loss: 0.58384770154953
(7242.42) Training batch 3000/7151 loss: 0.3446027934551239
(7370.89) Training batch 3500/7151 loss: 0.24444717168807983
(7499.39) Training batch 4000/7151 loss: 0.511130690574646
(7627.87) Training batch 4500/7151 loss: 0.1392657607793808
(7756.35) Training batch 5000/7151 loss: 0.010597693733870983
(7884.82) Training batch 5500/7151 loss: 0.13002127408981323
(8013.29) Training batch 6000/7151 loss: 0.168561190366745
(8141.77) Training batch 6500/7151 loss: 0.3580666780471802
(8270.31) Training batch 7000/7151 loss: 0.6206703186035156
(8308.99) Epoch 3 on training dataset: loss: 19928.394769725244; positive-loss: 17998.93; negative-loss: 1929.46 
(8308.99)------------- Training epoch 4 started -------------
(8541.91) Epoch 4 on test dataset: MRR: 0.42; Recalls: tensor([0.2943, 0.5514, 0.6628]) Loss: 39294.63positive-loss: 196.44; negative-loss: 39098.19 
(8542.18) Training batch 0/7151 loss: 2.2704052925109863
(8670.62) Training batch 500/7151 loss: 0.03083653189241886
(8799.06) Training batch 1000/7151 loss: 1.2814439535140991
(8927.49) Training batch 1500/7151 loss: 0.10278423875570297
(9055.94) Training batch 2000/7151 loss: 0.10253661125898361
(9184.39) Training batch 2500/7151 loss: 0.8154616355895996
(9312.82) Training batch 3000/7151 loss: 0.39292195439338684
(9441.26) Training batch 3500/7151 loss: 0.11558467149734497
(9569.71) Training batch 4000/7151 loss: 0.4452947974205017
(9698.14) Training batch 4500/7151 loss: 0.11519728600978851
(9826.58) Training batch 5000/7151 loss: 0.015348765067756176
(9955.03) Training batch 5500/7151 loss: 0.0620010569691658
(10083.48) Training batch 6000/7151 loss: 0.0946265459060669
(10211.91) Training batch 6500/7151 loss: 0.2647908926010132
(10340.36) Training batch 7000/7151 loss: 0.7595311999320984
(10378.75) Epoch 4 on training dataset: loss: 16914.468914294448; positive-loss: 15189.27; negative-loss: 1725.20 
(10378.75)------------- Training epoch 5 started -------------
(10610.06) Epoch 5 on test dataset: MRR: 0.44; Recalls: tensor([0.3203, 0.5658, 0.6751]) Loss: 40170.75positive-loss: 154.72; negative-loss: 40016.03 
(10610.33) Training batch 0/7151 loss: 0.6906812191009521
(10738.79) Training batch 500/7151 loss: 0.07616680860519409
(10867.25) Training batch 1000/7151 loss: 0.3331620991230011
(10995.70) Training batch 1500/7151 loss: 0.08531521260738373
(11124.16) Training batch 2000/7151 loss: 0.13483837246894836
(11252.62) Training batch 2500/7151 loss: 0.7417600154876709
(11381.08) Training batch 3000/7151 loss: 0.3689248263835907
(11509.54) Training batch 3500/7151 loss: 0.05973201245069504
(11638.00) Training batch 4000/7151 loss: 0.5210000872612
(11766.48) Training batch 4500/7151 loss: 0.031390730291604996
(11894.94) Training batch 5000/7151 loss: 0.2169942706823349
(12023.39) Training batch 5500/7151 loss: 0.7248491644859314
(12151.84) Training batch 6000/7151 loss: 0.1122925728559494
(12280.29) Training batch 6500/7151 loss: 0.5348695516586304
(12408.75) Training batch 7000/7151 loss: 0.46375149488449097
(12447.13) Epoch 5 on training dataset: loss: 14905.880468377756; positive-loss: 13332.91; negative-loss: 1572.97 
(12447.13)------------- Training epoch 6 started -------------
(12678.54) Epoch 6 on test dataset: MRR: 0.42; Recalls: tensor([0.2943, 0.5456, 0.6576]) Loss: 60327.40positive-loss: 100.47; negative-loss: 60226.93 
(12678.81) Training batch 0/7151 loss: 0.7824066877365112
(12807.25) Training batch 500/7151 loss: 0.07297684252262115
(12935.69) Training batch 1000/7151 loss: 0.4196722209453583
(13064.15) Training batch 1500/7151 loss: 0.17811813950538635
(13192.60) Training batch 2000/7151 loss: 0.0800400897860527
(13321.05) Training batch 2500/7151 loss: 0.790839672088623
(13449.50) Training batch 3000/7151 loss: 0.34557318687438965
(13577.96) Training batch 3500/7151 loss: 0.18846702575683594
(13706.40) Training batch 4000/7151 loss: 0.20827700197696686
(13834.84) Training batch 4500/7151 loss: 0.24627937376499176
(13963.28) Training batch 5000/7151 loss: 0.2761184871196747
(14091.73) Training batch 5500/7151 loss: 0.11660626530647278
(14220.18) Training batch 6000/7151 loss: 0.1147148534655571
(14348.64) Training batch 6500/7151 loss: 0.3103978633880615
(14477.10) Training batch 7000/7151 loss: 0.3534443974494934
(14515.48) Epoch 6 on training dataset: loss: 13354.826739517253; positive-loss: 11897.05; negative-loss: 1457.78 
(14515.48)------------- Training epoch 7 started -------------
(14746.76) Epoch 7 on test dataset: MRR: 0.42; Recalls: tensor([0.2956, 0.5488, 0.6628]) Loss: 41438.07positive-loss: 154.57; negative-loss: 41283.50 
(14747.03) Training batch 0/7151 loss: 0.5202664136886597
(14875.48) Training batch 500/7151 loss: 0.023299559950828552
(15003.93) Training batch 1000/7151 loss: 0.4021492004394531
(15132.38) Training batch 1500/7151 loss: 0.1352582424879074
(15260.82) Training batch 2000/7151 loss: 0.1139015480875969
(15389.27) Training batch 2500/7151 loss: 0.9415551424026489
(15517.72) Training batch 3000/7151 loss: 0.1344003826379776
(15646.17) Training batch 3500/7151 loss: 0.06837166845798492
(15774.62) Training batch 4000/7151 loss: 0.25999847054481506
(15903.06) Training batch 4500/7151 loss: 0.041236698627471924
(16031.53) Training batch 5000/7151 loss: 0.010447157546877861
(16159.98) Training batch 5500/7151 loss: 0.04934494197368622
(16288.42) Training batch 6000/7151 loss: 0.13394366204738617
(16416.87) Training batch 6500/7151 loss: 0.19496086239814758
(16545.32) Training batch 7000/7151 loss: 0.32946643233299255
(16583.70) Epoch 7 on training dataset: loss: 12005.627498997663; positive-loss: 10670.48; negative-loss: 1335.15 
(16583.70)------------- Training epoch 8 started -------------
(16815.08) Epoch 8 on test dataset: MRR: 0.44; Recalls: tensor([0.3145, 0.5710, 0.6745]) Loss: 57290.62positive-loss: 129.01; negative-loss: 57161.60 
(16815.34) Training batch 0/7151 loss: 0.1696411818265915
(16943.78) Training batch 500/7151 loss: 0.025732874870300293
(17072.22) Training batch 1000/7151 loss: 0.35476911067962646
(17200.67) Training batch 1500/7151 loss: 0.021461034193634987
(17329.13) Training batch 2000/7151 loss: 0.04214123263955116
(17457.59) Training batch 2500/7151 loss: 0.782071053981781
(17586.05) Training batch 3000/7151 loss: 0.31623730063438416
(17714.51) Training batch 3500/7151 loss: 0.761163592338562
(17842.96) Training batch 4000/7151 loss: 0.35264676809310913
(17971.41) Training batch 4500/7151 loss: 0.6724860072135925
(18099.86) Training batch 5000/7151 loss: 0.006322088651359081
(18228.31) Training batch 5500/7151 loss: 0.019093798473477364
(18356.76) Training batch 6000/7151 loss: 0.03803804889321327
(18485.23) Training batch 6500/7151 loss: 0.46286147832870483
(18613.69) Training batch 7000/7151 loss: 0.3646620213985443
(18652.07) Epoch 8 on training dataset: loss: 10857.149096537254; positive-loss: 9620.69; negative-loss: 1236.46 
(18652.07)------------- Training epoch 9 started -------------
(18883.32) Epoch 9 on test dataset: MRR: 0.44; Recalls: tensor([0.3171, 0.5690, 0.6725]) Loss: 62711.71positive-loss: 160.20; negative-loss: 62551.50 
(18883.58) Training batch 0/7151 loss: 0.088050976395607
(19012.05) Training batch 500/7151 loss: 0.006139650009572506
(19140.50) Training batch 1000/7151 loss: 0.2495589256286621
(19268.99) Training batch 1500/7151 loss: 0.14431819319725037
(19397.45) Training batch 2000/7151 loss: 0.42192161083221436
(19525.90) Training batch 2500/7151 loss: 0.47772663831710815
(19654.35) Training batch 3000/7151 loss: 0.2538440227508545
(19782.82) Training batch 3500/7151 loss: 0.1502089649438858
(19911.27) Training batch 4000/7151 loss: 0.3514561653137207
(20039.73) Training batch 4500/7151 loss: 0.005249317269772291
(20168.18) Training batch 5000/7151 loss: 0.0022591142915189266
(20296.64) Training batch 5500/7151 loss: 0.01375554222613573
(20425.10) Training batch 6000/7151 loss: 0.0852656364440918
(20553.59) Training batch 6500/7151 loss: 0.8820928335189819
(20682.09) Training batch 7000/7151 loss: 0.28128018975257874
(20720.48) Epoch 9 on training dataset: loss: 10005.310411648428; positive-loss: 8849.64; negative-loss: 1155.67 
(20720.48)------------- Training epoch 10 started -------------
(20951.85) Epoch 10 on test dataset: MRR: 0.43; Recalls: tensor([0.3164, 0.5540, 0.6419]) Loss: 67705.17positive-loss: 130.32; negative-loss: 67574.85 
(20952.12) Training batch 0/7151 loss: 0.17626900970935822
(21080.57) Training batch 500/7151 loss: 0.01082343328744173
(21209.02) Training batch 1000/7151 loss: 0.2392372488975525
(21337.47) Training batch 1500/7151 loss: 0.1300986409187317
(21465.93) Training batch 2000/7151 loss: 0.10862056910991669
(21594.37) Training batch 2500/7151 loss: 0.8409982919692993
(21722.84) Training batch 3000/7151 loss: 0.16982951760292053
(21851.30) Training batch 3500/7151 loss: 0.10084935277700424
(21979.75) Training batch 4000/7151 loss: 0.4572520852088928
(22108.19) Training batch 4500/7151 loss: 0.08156339079141617
(22236.64) Training batch 5000/7151 loss: 0.0020447808783501387
(22365.10) Training batch 5500/7151 loss: 0.04994305595755577
(22493.54) Training batch 6000/7151 loss: 0.003261235076934099
(22621.97) Training batch 6500/7151 loss: 0.11274063587188721
(22750.43) Training batch 7000/7151 loss: 0.2854771912097931
(22788.81) Epoch 10 on training dataset: loss: 9333.989161237214; positive-loss: 8237.63; negative-loss: 1096.36 
(22788.81)------------- Training epoch 11 started -------------
(23020.36) Epoch 11 on test dataset: MRR: 0.42; Recalls: tensor([0.3021, 0.5488, 0.6628]) Loss: 56354.78positive-loss: 185.17; negative-loss: 56169.61 
(23020.62) Training batch 0/7151 loss: 0.1361536979675293
(23149.10) Training batch 500/7151 loss: 0.007630222011357546
(23277.57) Training batch 1000/7151 loss: 0.19709883630275726
(23406.04) Training batch 1500/7151 loss: 0.012993390671908855
(23534.52) Training batch 2000/7151 loss: 0.14020106196403503
(23662.98) Training batch 2500/7151 loss: 0.41778188943862915
(23791.45) Training batch 3000/7151 loss: 0.21834422647953033
(23919.91) Training batch 3500/7151 loss: 0.041828904300928116
(24048.37) Training batch 4000/7151 loss: 0.24327948689460754
(24176.84) Training batch 4500/7151 loss: 0.048114754259586334
(24305.31) Training batch 5000/7151 loss: 0.004702070727944374
(24433.77) Training batch 5500/7151 loss: 0.010641584172844887
(24562.23) Training batch 6000/7151 loss: 0.03351101651787758
(24690.69) Training batch 6500/7151 loss: 0.09707753360271454
(24819.16) Training batch 7000/7151 loss: 0.19903378188610077
(24857.55) Epoch 11 on training dataset: loss: 8603.1331689289; positive-loss: 7572.49; negative-loss: 1030.65 
(24857.55)------------- Training epoch 12 started -------------
(25089.07) Epoch 12 on test dataset: MRR: 0.45; Recalls: tensor([0.3236, 0.5742, 0.6823]) Loss: 41087.75positive-loss: 183.11; negative-loss: 40904.63 
(25089.34) Training batch 0/7151 loss: 0.41421353816986084
(25217.80) Training batch 500/7151 loss: 0.02635067328810692
(25346.27) Training batch 1000/7151 loss: 0.44864484667778015
(25474.74) Training batch 1500/7151 loss: 0.03446584194898605
(25603.21) Training batch 2000/7151 loss: 0.0788189098238945
(25731.69) Training batch 2500/7151 loss: 0.4242440462112427
(25860.16) Training batch 3000/7151 loss: 0.10111774504184723
(25988.64) Training batch 3500/7151 loss: 0.022601323202252388
(26117.12) Training batch 4000/7151 loss: 0.1515592783689499
(26245.60) Training batch 4500/7151 loss: 0.07251746207475662
(26374.08) Training batch 5000/7151 loss: 0.011142447590827942
(26502.56) Training batch 5500/7151 loss: 0.014084938913583755
(26631.03) Training batch 6000/7151 loss: 0.049244772642850876
(26759.50) Training batch 6500/7151 loss: 0.49601370096206665
(26887.97) Training batch 7000/7151 loss: 0.06184005364775658
(26926.36) Epoch 12 on training dataset: loss: 8062.523744182449; positive-loss: 7114.27; negative-loss: 948.26 
(26926.36)------------- Training epoch 13 started -------------
(27158.35) Epoch 13 on test dataset: MRR: 0.45; Recalls: tensor([0.3314, 0.5885, 0.6829]) Loss: 44989.99positive-loss: 208.77; negative-loss: 44781.23 
(27158.62) Training batch 0/7151 loss: 0.28668469190597534
(27287.10) Training batch 500/7151 loss: 0.0045263622887432575
(27415.57) Training batch 1000/7151 loss: 0.4077725112438202
(27544.03) Training batch 1500/7151 loss: 0.0140654556453228
(27672.50) Training batch 2000/7151 loss: 0.07648874074220657
(27800.97) Training batch 2500/7151 loss: 0.5788259506225586
(27929.44) Training batch 3000/7151 loss: 0.3950751721858978
(28057.91) Training batch 3500/7151 loss: 0.1338784098625183
(28186.39) Training batch 4000/7151 loss: 0.21125765144824982
(28314.86) Training batch 4500/7151 loss: 0.3400954306125641
(28443.34) Training batch 5000/7151 loss: 0.0050494736060500145
(28571.84) Training batch 5500/7151 loss: 0.011243853718042374
(28701.72) Training batch 6000/7151 loss: 0.05542862042784691
(28830.35) Training batch 6500/7151 loss: 0.122321218252182
(28958.81) Training batch 7000/7151 loss: 0.10383524000644684
(28997.20) Epoch 13 on training dataset: loss: 7529.648722391794; positive-loss: 6622.35; negative-loss: 907.30 
(28997.20)------------- Training epoch 14 started -------------
(29229.06) Epoch 14 on test dataset: MRR: 0.44; Recalls: tensor([0.3236, 0.5632, 0.6569]) Loss: 46450.10positive-loss: 179.44; negative-loss: 46270.67 
(29229.33) Training batch 0/7151 loss: 0.49572014808654785
(29357.77) Training batch 500/7151 loss: 0.023496540263295174
(29486.21) Training batch 1000/7151 loss: 0.23481735587120056
(29614.64) Training batch 1500/7151 loss: 0.02925395593047142
(29743.07) Training batch 2000/7151 loss: 0.07821941375732422
(29871.51) Training batch 2500/7151 loss: 0.3138900697231293
(29999.95) Training batch 3000/7151 loss: 0.2033727616071701
(30128.39) Training batch 3500/7151 loss: 0.04326308146119118
(30256.83) Training batch 4000/7151 loss: 0.2797870337963104
(30385.27) Training batch 4500/7151 loss: 0.05325234308838844
(30513.72) Training batch 5000/7151 loss: 0.0015393805224448442
(30642.16) Training batch 5500/7151 loss: 0.037450775504112244
(30770.60) Training batch 6000/7151 loss: 0.0018184104701504111
(30899.03) Training batch 6500/7151 loss: 0.1302422434091568
(31027.48) Training batch 7000/7151 loss: 0.6611905694007874
(31065.86) Epoch 14 on training dataset: loss: 7148.4203030999; positive-loss: 6294.08; negative-loss: 854.34 
(31065.86)------------- Training epoch 15 started -------------
(31297.92) Epoch 15 on test dataset: MRR: 0.44; Recalls: tensor([0.3242, 0.5560, 0.6693]) Loss: 42666.87positive-loss: 190.36; negative-loss: 42476.51 
(31298.18) Training batch 0/7151 loss: 0.16193503141403198
(31426.64) Training batch 500/7151 loss: 0.0030113663524389267
(31555.10) Training batch 1000/7151 loss: 0.10267666727304459
(31683.55) Training batch 1500/7151 loss: 0.019707195460796356
(31812.00) Training batch 2000/7151 loss: 0.10626712441444397
(31940.45) Training batch 2500/7151 loss: 0.2801600694656372
(32068.92) Training batch 3000/7151 loss: 0.07542494684457779
(32197.40) Training batch 3500/7151 loss: 0.11037365347146988
(32325.88) Training batch 4000/7151 loss: 0.086820088326931
(32454.41) Training batch 4500/7151 loss: 0.03046795167028904
(32582.87) Training batch 5000/7151 loss: 0.004890839569270611
(32711.30) Training batch 5500/7151 loss: 0.005361523013561964
(32839.75) Training batch 6000/7151 loss: 0.002891447627916932
(32968.20) Training batch 6500/7151 loss: 0.19317901134490967
(33096.65) Training batch 7000/7151 loss: 0.3557181656360626
(33135.04) Epoch 15 on training dataset: loss: 6803.716090844505; positive-loss: 5987.99; negative-loss: 815.72 
(33135.04)------------- Training epoch 16 started -------------
(33366.97) Epoch 16 on test dataset: MRR: 0.44; Recalls: tensor([0.3164, 0.5677, 0.6673]) Loss: 65376.19positive-loss: 140.10; negative-loss: 65236.08 
(33367.24) Training batch 0/7151 loss: 0.09739730507135391
(33495.66) Training batch 500/7151 loss: 0.004310796968638897
(33624.09) Training batch 1000/7151 loss: 0.13361257314682007
(33752.52) Training batch 1500/7151 loss: 0.01562945917248726
(33880.95) Training batch 2000/7151 loss: 0.10900892317295074
(34009.38) Training batch 2500/7151 loss: 0.25719937682151794
(34137.82) Training batch 3000/7151 loss: 0.028237950056791306
(34266.27) Training batch 3500/7151 loss: 0.00847979262471199
(34394.71) Training batch 4000/7151 loss: 0.13164229691028595
(34523.16) Training batch 4500/7151 loss: 0.010527130216360092
(34651.61) Training batch 5000/7151 loss: 0.0029866958502680063
(34780.06) Training batch 5500/7151 loss: 0.0038676317781209946
(34908.51) Training batch 6000/7151 loss: 0.0013129840372130275
(35036.96) Training batch 6500/7151 loss: 0.1126270741224289
(35165.40) Training batch 7000/7151 loss: 0.04498371481895447
(35203.79) Epoch 16 on training dataset: loss: 6398.0749223573475; positive-loss: 5622.41; negative-loss: 775.66 
(35203.79)------------- Training epoch 17 started -------------
(35435.84) Epoch 17 on test dataset: MRR: 0.45; Recalls: tensor([0.3242, 0.5853, 0.6849]) Loss: 57249.65positive-loss: 186.47; negative-loss: 57063.18 
(35436.10) Training batch 0/7151 loss: 0.24556495249271393
(35564.55) Training batch 500/7151 loss: 0.00517958402633667
(35693.01) Training batch 1000/7151 loss: 0.4196426570415497
(35821.45) Training batch 1500/7151 loss: 0.010602414608001709
(35949.91) Training batch 2000/7151 loss: 0.0999741181731224
(36078.35) Training batch 2500/7151 loss: 0.2428186982870102
(36206.80) Training batch 3000/7151 loss: 0.09630128741264343
(36335.25) Training batch 3500/7151 loss: 0.009388848207890987
(36463.70) Training batch 4000/7151 loss: 0.06934008747339249
(36592.15) Training batch 4500/7151 loss: 0.0976630449295044
(36720.59) Training batch 5000/7151 loss: 0.19349463284015656
(36849.05) Training batch 5500/7151 loss: 0.010105634108185768
(36977.50) Training batch 6000/7151 loss: 0.005791919305920601
(37105.96) Training batch 6500/7151 loss: 0.06403257697820663
(37234.42) Training batch 7000/7151 loss: 0.11498749256134033
(37272.80) Epoch 17 on training dataset: loss: 6081.526053641758; positive-loss: 5345.97; negative-loss: 735.55 
(37272.80)------------- Training epoch 18 started -------------
(37504.53) Epoch 18 on test dataset: MRR: 0.44; Recalls: tensor([0.3262, 0.5527, 0.6393]) Loss: 61182.63positive-loss: 169.41; negative-loss: 61013.22 
(37504.80) Training batch 0/7151 loss: 0.21024249494075775
(37633.24) Training batch 500/7151 loss: 0.002434258349239826
(37761.67) Training batch 1000/7151 loss: 0.09557217359542847
(37890.11) Training batch 1500/7151 loss: 0.08365262299776077
(38018.55) Training batch 2000/7151 loss: 0.14275985956192017
(38146.99) Training batch 2500/7151 loss: 0.9334722757339478
(38275.43) Training batch 3000/7151 loss: 0.08301037549972534
(38403.87) Training batch 3500/7151 loss: 0.2537494897842407
(38532.30) Training batch 4000/7151 loss: 0.0831788033246994
(38660.74) Training batch 4500/7151 loss: 0.08402490615844727
(38789.18) Training batch 5000/7151 loss: 0.07946669310331345
(38917.62) Training batch 5500/7151 loss: 0.008077540434896946
(39046.07) Training batch 6000/7151 loss: 0.0011433189501985908
(39174.52) Training batch 6500/7151 loss: 0.09878218919038773
(39302.96) Training batch 7000/7151 loss: 0.08139286190271378
(39341.34) Epoch 18 on training dataset: loss: 5896.681024608648; positive-loss: 5193.08; negative-loss: 703.60 
(39341.34)------------- Training epoch 19 started -------------
(39573.64) Epoch 19 on test dataset: MRR: 0.45; Recalls: tensor([0.3197, 0.5781, 0.6803]) Loss: 52216.05positive-loss: 232.74; negative-loss: 51983.31 
(39573.90) Training batch 0/7151 loss: 0.20471705496311188
(39702.37) Training batch 500/7151 loss: 0.005671351682394743
(39830.84) Training batch 1000/7151 loss: 0.07861451804637909
(39959.29) Training batch 1500/7151 loss: 0.16646242141723633
(40087.76) Training batch 2000/7151 loss: 0.0889032781124115
(40216.22) Training batch 2500/7151 loss: 0.27645567059516907
(40344.68) Training batch 3000/7151 loss: 0.04763030633330345
(40473.14) Training batch 3500/7151 loss: 0.07131358236074448
(40601.61) Training batch 4000/7151 loss: 0.07741878926753998
(40730.07) Training batch 4500/7151 loss: 0.0021973622497171164
(40858.53) Training batch 5000/7151 loss: 0.005573085974901915
(40986.99) Training batch 5500/7151 loss: 0.136073037981987
(41115.45) Training batch 6000/7151 loss: 0.023806851357221603
(41243.92) Training batch 6500/7151 loss: 0.3018549680709839
(41372.38) Training batch 7000/7151 loss: 0.10683025419712067
(41410.77) Epoch 19 on training dataset: loss: 5494.676794195022; positive-loss: 4825.48; negative-loss: 669.20 
(41410.77)------------- Training epoch 20 started -------------
(41642.12) Epoch 20 on test dataset: MRR: 0.47; Recalls: tensor([0.3457, 0.6003, 0.6986]) Loss: 48049.70positive-loss: 187.95; negative-loss: 47861.75 
(41642.39) Training batch 0/7151 loss: 0.23111429810523987
(41770.83) Training batch 500/7151 loss: 0.014703965745866299
(41899.27) Training batch 1000/7151 loss: 0.04950207099318504
(42027.72) Training batch 1500/7151 loss: 0.036832574754953384
(42156.17) Training batch 2000/7151 loss: 0.07140787690877914
(42284.61) Training batch 2500/7151 loss: 0.6027563810348511
(42413.06) Training batch 3000/7151 loss: 0.05073046311736107
(42541.51) Training batch 3500/7151 loss: 0.054187558591365814
(42669.96) Training batch 4000/7151 loss: 0.4939776360988617
(42798.41) Training batch 4500/7151 loss: 0.03332682326436043
(42926.85) Training batch 5000/7151 loss: 0.003695709863677621
(43055.30) Training batch 5500/7151 loss: 0.11415785551071167
(43183.74) Training batch 6000/7151 loss: 0.036157913506031036
(43312.19) Training batch 6500/7151 loss: 0.032571326941251755
(43440.64) Training batch 7000/7151 loss: 0.07146114856004715
(43479.04) Epoch 20 on training dataset: loss: 5260.2931338090975; positive-loss: 4617.69; negative-loss: 642.60 
(43479.04)------------- Training epoch 21 started -------------
(43710.27) Epoch 21 on test dataset: MRR: 0.46; Recalls: tensor([0.3359, 0.6009, 0.6882]) Loss: 49390.55positive-loss: 245.10; negative-loss: 49145.44 
(43710.53) Training batch 0/7151 loss: 0.050152234733104706
(43838.99) Training batch 500/7151 loss: 0.000425473612267524
(43967.43) Training batch 1000/7151 loss: 0.2673296630382538
(44095.87) Training batch 1500/7151 loss: 0.4665863513946533
(44224.32) Training batch 2000/7151 loss: 0.1326686441898346
(44352.77) Training batch 2500/7151 loss: 0.4099300503730774
(44481.21) Training batch 3000/7151 loss: 0.03856324777007103
(44609.65) Training batch 3500/7151 loss: 0.04987657070159912
(44738.09) Training batch 4000/7151 loss: 0.23721137642860413
(44866.55) Training batch 4500/7151 loss: 0.004285768140107393
(44995.00) Training batch 5000/7151 loss: 0.02051146887242794
(45123.44) Training batch 5500/7151 loss: 0.0010095665929839015
(45251.88) Training batch 6000/7151 loss: 0.03493078425526619
(45380.33) Training batch 6500/7151 loss: 0.024331411346793175
(45508.77) Training batch 7000/7151 loss: 0.036813490092754364
(45547.15) Epoch 21 on training dataset: loss: 5047.0378129575365; positive-loss: 4434.43; negative-loss: 612.61 
(45547.15)------------- Training epoch 22 started -------------
(45778.52) Epoch 22 on test dataset: MRR: 0.46; Recalls: tensor([0.3333, 0.5944, 0.6927]) Loss: 50604.93positive-loss: 205.49; negative-loss: 50399.43 
(45778.78) Training batch 0/7151 loss: 0.10356669872999191
(45907.23) Training batch 500/7151 loss: 0.003836089977994561
(46035.69) Training batch 1000/7151 loss: 0.2223861813545227
(46164.14) Training batch 1500/7151 loss: 0.011014611460268497
(46292.59) Training batch 2000/7151 loss: 0.12633304297924042
(46421.04) Training batch 2500/7151 loss: 0.41717928647994995
(46549.49) Training batch 3000/7151 loss: 0.07375121861696243
(46677.94) Training batch 3500/7151 loss: 0.011892388574779034
(46806.39) Training batch 4000/7151 loss: 0.03719458729028702
(46934.85) Training batch 4500/7151 loss: 0.010769070126116276
(47063.31) Training batch 5000/7151 loss: 0.0005536007229238749
(47191.77) Training batch 5500/7151 loss: 0.015458562411367893
(47320.23) Training batch 6000/7151 loss: 0.0006381102139130235
(47448.67) Training batch 6500/7151 loss: 0.13407497107982635
(47577.12) Training batch 7000/7151 loss: 0.21037183701992035
(47615.50) Epoch 22 on training dataset: loss: 4978.350068007536; positive-loss: 4380.69; negative-loss: 597.66 
(47615.50)------------- Training epoch 23 started -------------
(47846.72) Epoch 23 on test dataset: MRR: 0.47; Recalls: tensor([0.3424, 0.6029, 0.6875]) Loss: 42415.77positive-loss: 273.97; negative-loss: 42141.80 
(47846.99) Training batch 0/7151 loss: 0.13889601826667786
(47975.44) Training batch 500/7151 loss: 0.0016727469628676772
(48103.87) Training batch 1000/7151 loss: 0.03197919577360153
(48232.31) Training batch 1500/7151 loss: 0.015917174518108368
(48360.74) Training batch 2000/7151 loss: 0.07110433280467987
(48489.18) Training batch 2500/7151 loss: 2.3516106605529785
(48617.63) Training batch 3000/7151 loss: 0.027260644361376762
(48746.13) Training batch 3500/7151 loss: 0.03701281547546387
(48875.69) Training batch 4000/7151 loss: 0.030457941815257072
(49004.31) Training batch 4500/7151 loss: 0.004052573349326849
(49132.74) Training batch 5000/7151 loss: 0.0812227725982666
(49261.18) Training batch 5500/7151 loss: 0.09576233476400375
(49389.62) Training batch 6000/7151 loss: 0.005770782008767128
(49518.06) Training batch 6500/7151 loss: 0.10423404723405838
(49646.50) Training batch 7000/7151 loss: 0.09334929287433624
(49684.88) Epoch 23 on training dataset: loss: 4855.149620190419; positive-loss: 4277.13; negative-loss: 578.02 
(49684.88)------------- Training epoch 24 started -------------
(49916.33) Epoch 24 on test dataset: MRR: 0.43; Recalls: tensor([0.3034, 0.5749, 0.6777]) Loss: 55121.82positive-loss: 118.14; negative-loss: 55003.68 
(49916.60) Training batch 0/7151 loss: 0.4378131330013275
(50045.09) Training batch 500/7151 loss: 0.0016789402579888701
(50173.70) Training batch 1000/7151 loss: 0.05028413608670235
(50302.29) Training batch 1500/7151 loss: 0.01760406605899334
(50430.92) Training batch 2000/7151 loss: 0.14444072544574738
(50559.59) Training batch 2500/7151 loss: 0.6171900033950806
(50688.48) Training batch 3000/7151 loss: 0.1378907859325409
(50817.36) Training batch 3500/7151 loss: 0.023413795977830887
(50946.30) Training batch 4000/7151 loss: 0.013976135291159153
(51075.27) Training batch 4500/7151 loss: 0.00019691276247613132
(51204.23) Training batch 5000/7151 loss: 0.006685421336442232
(51333.17) Training batch 5500/7151 loss: 0.011896188370883465
(51462.13) Training batch 6000/7151 loss: 0.0003128649841528386
(51591.10) Training batch 6500/7151 loss: 0.04978317767381668
(51720.08) Training batch 7000/7151 loss: 0.10550932586193085
(51758.64) Epoch 24 on training dataset: loss: 4610.259825169467; positive-loss: 4061.47; negative-loss: 548.79 
(51758.64)------------- Training epoch 25 started -------------
(51990.51) Epoch 25 on test dataset: MRR: 0.43; Recalls: tensor([0.3197, 0.5521, 0.6178]) Loss: 61021.75positive-loss: 203.01; negative-loss: 60818.75 
(51990.78) Training batch 0/7151 loss: 0.1685408055782318
(52119.23) Training batch 500/7151 loss: 0.0015982247423380613
(52247.69) Training batch 1000/7151 loss: 1.5456913709640503
(52376.13) Training batch 1500/7151 loss: 0.05317126587033272
(52504.57) Training batch 2000/7151 loss: 0.06351317465305328
(52633.01) Training batch 2500/7151 loss: 0.47412052750587463
(52761.51) Training batch 3000/7151 loss: 0.06218183413147926
(52890.12) Training batch 3500/7151 loss: 0.008501914329826832
(53018.67) Training batch 4000/7151 loss: 0.004942358937114477
(53149.21) Training batch 4500/7151 loss: 0.0005368606653064489
(53277.66) Training batch 5000/7151 loss: 0.000998014584183693
(53406.11) Training batch 5500/7151 loss: 0.0020189641509205103
(53534.55) Training batch 6000/7151 loss: 0.0027259693015366793
(53663.00) Training batch 6500/7151 loss: 0.17707958817481995
(53791.45) Training batch 7000/7151 loss: 0.11914463341236115
(53829.84) Epoch 25 on training dataset: loss: 4384.674339999625; positive-loss: 3853.49; negative-loss: 531.18 
(53829.84)------------- Training epoch 26 started -------------
(54061.96) Epoch 26 on test dataset: MRR: 0.44; Recalls: tensor([0.3320, 0.5514, 0.6094]) Loss: 63651.97positive-loss: 175.94; negative-loss: 63476.03 
(54062.22) Training batch 0/7151 loss: 0.039781395345926285
(54190.71) Training batch 500/7151 loss: 0.0009270913433283567
(54319.21) Training batch 1000/7151 loss: 0.27666813135147095
(54447.88) Training batch 1500/7151 loss: 0.002837634179741144
(54576.88) Training batch 2000/7151 loss: 0.11580350250005722
(54705.83) Training batch 2500/7151 loss: 0.5619922280311584
(54834.84) Training batch 3000/7151 loss: 0.15887951850891113
(54963.84) Training batch 3500/7151 loss: 0.0032938525546342134
(55092.85) Training batch 4000/7151 loss: 0.07532224804162979
(55221.77) Training batch 4500/7151 loss: 0.10290578007698059
(55350.25) Training batch 5000/7151 loss: 0.00224317261017859
(55478.72) Training batch 5500/7151 loss: 0.014629551209509373
(55607.22) Training batch 6000/7151 loss: 0.0015647970139980316
(55735.73) Training batch 6500/7151 loss: 0.11406541615724564
(55864.21) Training batch 7000/7151 loss: 0.12005262076854706
(55902.60) Epoch 26 on training dataset: loss: 4275.524768925445; positive-loss: 3779.53; negative-loss: 496.00 
(55902.60)------------- Training epoch 27 started -------------
(56134.27) Epoch 27 on test dataset: MRR: 0.47; Recalls: tensor([0.3477, 0.6029, 0.6888]) Loss: 51465.22positive-loss: 198.41; negative-loss: 51266.81 
(56134.55) Training batch 0/7151 loss: 0.1532127857208252
(56263.00) Training batch 500/7151 loss: 0.0005703946808353066
(56391.43) Training batch 1000/7151 loss: 0.2250070869922638
(56519.88) Training batch 1500/7151 loss: 0.010480809025466442
(56648.33) Training batch 2000/7151 loss: 0.18903328478336334
(56776.77) Training batch 2500/7151 loss: 0.3052545487880707
(56905.23) Training batch 3000/7151 loss: 0.01653064414858818
(57033.67) Training batch 3500/7151 loss: 0.0021330511663109064
(57162.11) Training batch 4000/7151 loss: 0.041700802743434906
(57290.59) Training batch 4500/7151 loss: 0.014754974283277988
(57419.24) Training batch 5000/7151 loss: 0.09717260301113129
(57547.95) Training batch 5500/7151 loss: 0.011285312473773956
(57676.55) Training batch 6000/7151 loss: 0.0003070558013860136
(57805.15) Training batch 6500/7151 loss: 0.11798103898763657
(57933.69) Training batch 7000/7151 loss: 0.2955496311187744
(57972.10) Epoch 27 on training dataset: loss: 4254.444869743706; positive-loss: 3755.99; negative-loss: 498.45 
(57972.10)------------- Training epoch 28 started -------------
(58203.42) Epoch 28 on test dataset: MRR: 0.47; Recalls: tensor([0.3503, 0.6061, 0.6829]) Loss: 48066.72positive-loss: 183.93; negative-loss: 47882.78 
(58203.68) Training batch 0/7151 loss: 0.03906337171792984
(58332.28) Training batch 500/7151 loss: 0.0031433003023266792
(58460.98) Training batch 1000/7151 loss: 0.02698184736073017
(58589.64) Training batch 1500/7151 loss: 0.003958518151193857
(58718.34) Training batch 2000/7151 loss: 0.10462736338376999
(58846.92) Training batch 2500/7151 loss: 0.29104480147361755
(58975.57) Training batch 3000/7151 loss: 0.060533005744218826
(59104.10) Training batch 3500/7151 loss: 0.02912716381251812
(59232.66) Training batch 4000/7151 loss: 0.04245379939675331
(59361.19) Training batch 4500/7151 loss: 0.0031108029652386904
(59489.70) Training batch 5000/7151 loss: 0.0015676214825361967
(59618.24) Training batch 5500/7151 loss: 0.006163047626614571
(59746.81) Training batch 6000/7151 loss: 0.0030435577500611544
(59875.46) Training batch 6500/7151 loss: 0.1463189423084259
(60004.17) Training batch 7000/7151 loss: 0.06486064940690994
(60042.60) Epoch 28 on training dataset: loss: 3889.1090392696306; positive-loss: 3415.74; negative-loss: 473.37 
(60042.60)------------- Training epoch 29 started -------------
(60274.69) Epoch 29 on test dataset: MRR: 0.45; Recalls: tensor([0.3320, 0.5846, 0.6673]) Loss: 55048.62positive-loss: 155.33; negative-loss: 54893.29 
(60274.95) Training batch 0/7151 loss: 0.0804029330611229
(60403.55) Training batch 500/7151 loss: 0.0010750272776931524
(60532.23) Training batch 1000/7151 loss: 0.030115678906440735
(60660.88) Training batch 1500/7151 loss: 0.003650476224720478
(60789.42) Training batch 2000/7151 loss: 0.1751929074525833
(60918.09) Training batch 2500/7151 loss: 0.24542616307735443
(61046.70) Training batch 3000/7151 loss: 0.10067757964134216
(61175.40) Training batch 3500/7151 loss: 0.009591484442353249
(61304.07) Training batch 4000/7151 loss: 0.09166078269481659
(61432.66) Training batch 4500/7151 loss: 0.0026778161991387606
(61561.31) Training batch 5000/7151 loss: 0.0013249454787001014
(61689.89) Training batch 5500/7151 loss: 0.12380552291870117
(61818.44) Training batch 6000/7151 loss: 0.0002478341048117727
(61946.96) Training batch 6500/7151 loss: 0.1735830157995224
(62075.49) Training batch 7000/7151 loss: 0.008857307955622673
(62113.94) Epoch 29 on training dataset: loss: 4002.9423395383246; positive-loss: 3529.04; negative-loss: 473.90 
Model saved to ce_lr1e-5_wd1e-1_dr0.1.pt
(63412.40) Final test on test dataset!MRR: 0.44; Recalls: tensor([0.3333, 0.5547, 0.6250]) Loss: 58585.26positive-loss: 44.79; negative-loss: 58540.47 
