Current time: 2024-06-05_13-29-32
train: True
num_epochs: 30
stop_time: None
load_model_path: None
save_model_path: cross_encoder.pt
bert_model_name: cross-encoder/ms-marco-MiniLM-L-6-v2
lr: 1e-05
weight_decay: 0.01
dropout_rate: 0.1
label_smoothing: 0
gradient_clip: 0
compute_recall_at_k: False
(1.65)------------- Training epoch 0 started -------------
(211.28) Epoch 0 on test dataset: MRR: 0.25; Recalls: tensor([0.1236, 0.3752, 0.5642]) Loss: 27447.49positive-loss: 2858.16; negative-loss: 24589.32 
(211.43) Training batch 0/7151 loss: 1.268429160118103
(247.24) Training batch 500/7151 loss: 0.43691903352737427
(282.96) Training batch 1000/7151 loss: 1.0214195251464844
(314.40) Training batch 1500/7151 loss: 0.3211992681026459
(339.45) Training batch 2000/7151 loss: 0.5484858751296997
(364.99) Training batch 2500/7151 loss: 1.1238771677017212
(389.97) Training batch 3000/7151 loss: 0.5465217232704163
(415.00) Training batch 3500/7151 loss: 0.4794268012046814
(439.94) Training batch 4000/7151 loss: 0.9541015625
(464.88) Training batch 4500/7151 loss: 0.48164814710617065
(490.42) Training batch 5000/7151 loss: 0.4964611232280731
(517.70) Training batch 5500/7151 loss: 0.6080005764961243
(553.40) Training batch 6000/7151 loss: 0.45218491554260254
(589.17) Training batch 6500/7151 loss: 1.4012470245361328
(625.02) Training batch 7000/7151 loss: 0.8323065042495728
(635.76) Epoch 0 on training dataset: loss: 35869.74267234653; positive-loss: 32687.12; negative-loss: 3182.62 
(635.76)------------- Training epoch 1 started -------------
(845.83) Epoch 1 on test dataset: MRR: 0.54; Recalls: tensor([0.4055, 0.6930, 0.7841]) Loss: 27321.96positive-loss: 1084.38; negative-loss: 26237.59 
(845.91) Training batch 0/7151 loss: 2.556596517562866
(881.58) Training batch 500/7151 loss: 0.5605881810188293
(917.22) Training batch 1000/7151 loss: 0.7311765551567078
(948.27) Training batch 1500/7151 loss: 0.17706969380378723
(973.25) Training batch 2000/7151 loss: 0.452987402677536
(998.22) Training batch 2500/7151 loss: 1.0063825845718384
(1023.16) Training batch 3000/7151 loss: 0.480034202337265
(1048.35) Training batch 3500/7151 loss: 0.1188025176525116
(1073.37) Training batch 4000/7151 loss: 0.8756193518638611
(1098.40) Training batch 4500/7151 loss: 0.7309494614601135
(1123.41) Training batch 5000/7151 loss: 0.4305657744407654
(1150.08) Training batch 5500/7151 loss: 0.3363020718097687
(1186.04) Training batch 6000/7151 loss: 0.24954062700271606
(1221.90) Training batch 6500/7151 loss: 1.4310321807861328
Early stopping by user Ctrl+C interaction.
