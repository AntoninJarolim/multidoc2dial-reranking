Model loaded successfully from ce_7100.pt
(3.69)------------- Training epoch 0 started -------------
(247.54) Epoch 0 on test dataset: MRR: 0.45; Recalls: tensor([0.3197, 0.5944, 0.6953]) Loss: 50287.75positive-loss: 245.75; negative-loss: 50042.00 
(247.84) Training batch 0/7151 loss: 0.7205623984336853
(388.45) Training batch 500/7151 loss: 0.010446867905557156
(529.25) Training batch 1000/7151 loss: 0.24492059648036957
(670.08) Training batch 1500/7151 loss: 0.0015199974877759814
(810.95) Training batch 2000/7151 loss: 0.049372706562280655
(951.90) Training batch 2500/7151 loss: 0.11022207140922546
(1092.87) Training batch 3000/7151 loss: 0.1877712607383728
(1233.88) Training batch 3500/7151 loss: 0.00862740259617567
(1374.82) Training batch 4000/7151 loss: 0.04617610201239586
(1515.61) Training batch 4500/7151 loss: 0.3341566324234009
(1656.25) Training batch 5000/7151 loss: 0.0038714392576366663
(1796.80) Training batch 5500/7151 loss: 0.00303347478620708
(1937.30) Training batch 6000/7151 loss: 0.04464278742671013
(2077.74) Training batch 6500/7151 loss: 0.33954378962516785
(2218.19) Training batch 7000/7151 loss: 0.14534306526184082
(2260.16) Epoch 0 on training dataset: loss: 4597.373352732749; positive-loss: 4053.11; negative-loss: 544.26 
(2260.16)------------- Training epoch 1 started -------------
(2505.33) Epoch 1 on test dataset: MRR: 0.45; Recalls: tensor([0.3197, 0.6016, 0.7038]) Loss: 58905.25positive-loss: 136.58; negative-loss: 58768.67 
(2505.62) Training batch 0/7151 loss: 0.3661811351776123
(2645.97) Training batch 500/7151 loss: 0.003902379423379898
(2786.35) Training batch 1000/7151 loss: 0.12339190393686295
(2926.68) Training batch 1500/7151 loss: 0.0035031745210289955
(3066.98) Training batch 2000/7151 loss: 0.0849510207772255
(3207.35) Training batch 2500/7151 loss: 0.320220023393631
(3347.69) Training batch 3000/7151 loss: 0.020534757524728775
(3488.01) Training batch 3500/7151 loss: 0.05613896623253822
(3628.32) Training batch 4000/7151 loss: 0.019972803071141243
(3768.64) Training batch 4500/7151 loss: 0.1580810844898224
(3909.00) Training batch 5000/7151 loss: 0.011264797300100327
(4049.29) Training batch 5500/7151 loss: 0.044994499534368515
(4189.59) Training batch 6000/7151 loss: 0.008679765276610851
(4329.86) Training batch 6500/7151 loss: 0.4346904754638672
(4470.19) Training batch 7000/7151 loss: 0.021350368857383728
(4512.13) Epoch 1 on training dataset: loss: 4552.791097715223; positive-loss: 4016.43; negative-loss: 536.36 
(4512.13)------------- Training epoch 2 started -------------
(4757.17) Epoch 2 on test dataset: MRR: 0.45; Recalls: tensor([0.3242, 0.5970, 0.7077]) Loss: 53133.24positive-loss: 218.95; negative-loss: 52914.29 
(4757.45) Training batch 0/7151 loss: 0.14502103626728058
(4897.69) Training batch 500/7151 loss: 0.04224005714058876
(5037.99) Training batch 1000/7151 loss: 0.11100626736879349
(5178.25) Training batch 1500/7151 loss: 0.002634183969348669
(5318.50) Training batch 2000/7151 loss: 0.01371584553271532
(5458.79) Training batch 2500/7151 loss: 0.06085824593901634
(5599.07) Training batch 3000/7151 loss: 0.011613775976002216
(5739.31) Training batch 3500/7151 loss: 0.004693773575127125
(5879.57) Training batch 4000/7151 loss: 0.047143809497356415
(6019.78) Training batch 4500/7151 loss: 0.0005220503662712872
(6160.03) Training batch 5000/7151 loss: 0.0032628884073346853
(6300.31) Training batch 5500/7151 loss: 0.002071051625534892
(6440.56) Training batch 6000/7151 loss: 0.0019333852687850595
(6580.79) Training batch 6500/7151 loss: 0.14577408134937286
(6721.10) Training batch 7000/7151 loss: 0.25288230180740356
(6763.06) Epoch 2 on training dataset: loss: 4312.034180557697; positive-loss: 3796.48; negative-loss: 515.55 
(6763.06)------------- Training epoch 3 started -------------
(7008.37) Epoch 3 on test dataset: MRR: 0.44; Recalls: tensor([0.3118, 0.5794, 0.6999]) Loss: 54125.30positive-loss: 205.94; negative-loss: 53919.36 
(7008.66) Training batch 0/7151 loss: 0.20090050995349884
(7149.27) Training batch 500/7151 loss: 0.004511226434260607
(7290.01) Training batch 1000/7151 loss: 0.07845572382211685
(7430.76) Training batch 1500/7151 loss: 0.0009970783721655607
(7571.54) Training batch 2000/7151 loss: 0.20805945992469788
(7712.37) Training batch 2500/7151 loss: 0.3115118443965912
(7853.23) Training batch 3000/7151 loss: 0.08516482263803482
(7994.13) Training batch 3500/7151 loss: 0.012842684052884579
(8135.03) Training batch 4000/7151 loss: 0.08419009298086166
(8275.88) Training batch 4500/7151 loss: 0.0015687694540247321
(8416.64) Training batch 5000/7151 loss: 0.004044926725327969
(8557.25) Training batch 5500/7151 loss: 0.009390350431203842
(8697.77) Training batch 6000/7151 loss: 0.0004995765630155802
(8838.19) Training batch 6500/7151 loss: 0.045555535703897476
(8978.55) Training batch 7000/7151 loss: 0.05849538370966911
(9020.52) Epoch 3 on training dataset: loss: 4180.354972979723; positive-loss: 3688.43; negative-loss: 491.93 
(9020.52)------------- Training epoch 4 started -------------
(9265.56) Epoch 4 on test dataset: MRR: 0.45; Recalls: tensor([0.3288, 0.5781, 0.6621]) Loss: 55739.13positive-loss: 237.56; negative-loss: 55501.57 
(9265.84) Training batch 0/7151 loss: 0.11256378889083862
(9406.13) Training batch 500/7151 loss: 0.006493980996310711
(9546.46) Training batch 1000/7151 loss: 0.2587778866291046
(9686.76) Training batch 1500/7151 loss: 0.0013005002401769161
(9827.08) Training batch 2000/7151 loss: 0.06251657009124756
(9967.39) Training batch 2500/7151 loss: 0.11862360686063766
(10107.68) Training batch 3000/7151 loss: 0.022453680634498596
(10247.97) Training batch 3500/7151 loss: 0.006746081635355949
(10388.33) Training batch 4000/7151 loss: 0.00993269868195057
(10528.81) Training batch 4500/7151 loss: 0.0028959745541214943
(10669.43) Training batch 5000/7151 loss: 0.007267373614013195
(10810.15) Training batch 5500/7151 loss: 0.003394157625734806
(10950.97) Training batch 6000/7151 loss: 0.0006311783217824996
(11091.75) Training batch 6500/7151 loss: 0.03918306529521942
(11232.62) Training batch 7000/7151 loss: 0.01455006655305624
(11274.72) Epoch 4 on training dataset: loss: 4175.681010854186; positive-loss: 3686.54; negative-loss: 489.14 
(11274.72)------------- Training epoch 5 started -------------
(11520.75) Epoch 5 on test dataset: MRR: 0.47; Recalls: tensor([0.3438, 0.6094, 0.7259]) Loss: 45044.88positive-loss: 207.19; negative-loss: 44837.68 
(11521.03) Training batch 0/7151 loss: 0.03197728469967842
(11661.97) Training batch 500/7151 loss: 0.0009724803967401385
(11802.94) Training batch 1000/7151 loss: 0.055603690445423126
(11943.95) Training batch 1500/7151 loss: 0.027071529999375343
(12084.99) Training batch 2000/7151 loss: 0.16292795538902283
(12226.04) Training batch 2500/7151 loss: 0.08511727303266525
(12367.08) Training batch 3000/7151 loss: 0.013254133984446526
(12508.12) Training batch 3500/7151 loss: 0.01588650979101658
(12649.20) Training batch 4000/7151 loss: 0.024718107655644417
(12790.26) Training batch 4500/7151 loss: 0.0018073056126013398
(12931.34) Training batch 5000/7151 loss: 0.022668231278657913
(13072.45) Training batch 5500/7151 loss: 0.0612250491976738
(13213.56) Training batch 6000/7151 loss: 0.003219671780243516
(13354.65) Training batch 6500/7151 loss: 0.22714798152446747
(13495.76) Training batch 7000/7151 loss: 0.008115969598293304
(13537.92) Epoch 5 on training dataset: loss: 4052.7090875386393; positive-loss: 3565.77; negative-loss: 486.94 
(13537.92)------------- Training epoch 6 started -------------
(13784.37) Epoch 6 on test dataset: MRR: 0.46; Recalls: tensor([0.3327, 0.5970, 0.6855]) Loss: 53934.95positive-loss: 164.34; negative-loss: 53770.61 
(13784.65) Training batch 0/7151 loss: 0.04729141294956207
(13925.77) Training batch 500/7151 loss: 0.005178720224648714
(14066.95) Training batch 1000/7151 loss: 0.16851115226745605
(14208.09) Training batch 1500/7151 loss: 0.05080604553222656
(14349.31) Training batch 2000/7151 loss: 0.1822090744972229
(14490.51) Training batch 2500/7151 loss: 0.1522921770811081
(14631.67) Training batch 3000/7151 loss: 0.029241664335131645
(14772.84) Training batch 3500/7151 loss: 0.0025825724005699158
(14914.01) Training batch 4000/7151 loss: 0.11618872731924057
(15055.22) Training batch 4500/7151 loss: 0.001527758315205574
(15196.42) Training batch 5000/7151 loss: 0.013079676777124405
(15337.66) Training batch 5500/7151 loss: 0.0010194587521255016
(15478.92) Training batch 6000/7151 loss: 0.007882349193096161
(15620.12) Training batch 6500/7151 loss: 0.056023724377155304
(15761.35) Training batch 7000/7151 loss: 0.14249573647975922
(15803.59) Epoch 6 on training dataset: loss: 3890.4984359643604; positive-loss: 3420.01; negative-loss: 470.49 
(15803.59)------------- Training epoch 7 started -------------
(16049.94) Epoch 7 on test dataset: MRR: 0.46; Recalls: tensor([0.3301, 0.5892, 0.7103]) Loss: 61985.68positive-loss: 131.99; negative-loss: 61853.69 
(16050.22) Training batch 0/7151 loss: 0.02918786182999611
(16191.40) Training batch 500/7151 loss: 0.00043524024658836424
(16332.66) Training batch 1000/7151 loss: 1.4585520029067993
(16473.86) Training batch 1500/7151 loss: 0.007401485461741686
(16615.08) Training batch 2000/7151 loss: 0.04770691320300102
(16756.30) Training batch 2500/7151 loss: 0.3438596725463867
(16897.51) Training batch 3000/7151 loss: 0.03717288747429848
(17038.76) Training batch 3500/7151 loss: 0.000666251580696553
(17180.03) Training batch 4000/7151 loss: 0.137751042842865
(17321.30) Training batch 4500/7151 loss: 0.0027861292473971844
(17462.56) Training batch 5000/7151 loss: 0.0035595842637121677
(17603.79) Training batch 5500/7151 loss: 0.06407177448272705
(17745.05) Training batch 6000/7151 loss: 0.010755208320915699
(17886.27) Training batch 6500/7151 loss: 0.057783517986536026
(18027.56) Training batch 7000/7151 loss: 0.00472936499863863
(18069.79) Epoch 7 on training dataset: loss: 3932.2200056632537; positive-loss: 3460.52; negative-loss: 471.70 
(18069.79)------------- Training epoch 8 started -------------
(18316.37) Epoch 8 on test dataset: MRR: 0.46; Recalls: tensor([0.3366, 0.6126, 0.6934]) Loss: 55284.09positive-loss: 205.46; negative-loss: 55078.63 
(18316.66) Training batch 0/7151 loss: 0.03474050760269165
(18457.83) Training batch 500/7151 loss: 0.0017063175328075886
(18599.10) Training batch 1000/7151 loss: 0.06745975464582443
(18740.40) Training batch 1500/7151 loss: 0.016103586181998253
(18881.70) Training batch 2000/7151 loss: 0.08120041340589523
(19023.01) Training batch 2500/7151 loss: 0.08986338973045349
(19164.27) Training batch 3000/7151 loss: 0.12951377034187317
(19305.59) Training batch 3500/7151 loss: 0.0032794892322272062
(19446.90) Training batch 4000/7151 loss: 0.05858375132083893
(19588.17) Training batch 4500/7151 loss: 0.013917236588895321
(19729.47) Training batch 5000/7151 loss: 0.00278795906342566
(19870.77) Training batch 5500/7151 loss: 0.0071258037351071835
(20012.11) Training batch 6000/7151 loss: 0.00017935683717951179
(20153.40) Training batch 6500/7151 loss: 0.1308618187904358
(20294.80) Training batch 7000/7151 loss: 0.01665685325860977
(20337.06) Epoch 8 on training dataset: loss: 3704.191057901522; positive-loss: 3259.10; negative-loss: 445.09 
(20337.06)------------- Training epoch 9 started -------------
(20583.69) Epoch 9 on test dataset: MRR: 0.46; Recalls: tensor([0.3294, 0.5957, 0.7090]) Loss: 56272.71positive-loss: 199.04; negative-loss: 56073.67 
(20583.98) Training batch 0/7151 loss: 0.06149570271372795
(20725.22) Training batch 500/7151 loss: 0.00822885800153017
(20866.64) Training batch 1000/7151 loss: 0.1056426465511322
(21008.03) Training batch 1500/7151 loss: 0.0013076582690700889
(21149.42) Training batch 2000/7151 loss: 0.041061971336603165
(21290.89) Training batch 2500/7151 loss: 0.013678138144314289
(21432.33) Training batch 3000/7151 loss: 0.1209719181060791
(21573.73) Training batch 3500/7151 loss: 0.01155154686421156
(21715.08) Training batch 4000/7151 loss: 0.05376890301704407
(21856.51) Training batch 4500/7151 loss: 0.0011013590265065432
(21997.91) Training batch 5000/7151 loss: 0.006965110544115305
(22139.28) Training batch 5500/7151 loss: 0.007103330455720425
(22280.63) Training batch 6000/7151 loss: 0.0003646961704362184
(22421.96) Training batch 6500/7151 loss: 0.7380194664001465
(22563.33) Training batch 7000/7151 loss: 0.030810639262199402
(22605.59) Epoch 9 on training dataset: loss: 3709.248375631598; positive-loss: 3268.56; negative-loss: 440.69 
(22605.59)------------- Training epoch 10 started -------------
(22852.08) Epoch 10 on test dataset: MRR: 0.49; Recalls: tensor([0.3626, 0.6354, 0.7318]) Loss: 32755.54positive-loss: 377.41; negative-loss: 32378.13 
(22852.37) Training batch 0/7151 loss: 0.042339447885751724
(22993.66) Training batch 500/7151 loss: 0.0017309304093942046
(23135.02) Training batch 1000/7151 loss: 0.2987824082374573
(23276.39) Training batch 1500/7151 loss: 0.1183638870716095
(23417.79) Training batch 2000/7151 loss: 0.08295407891273499
(23559.17) Training batch 2500/7151 loss: 0.10956234484910965
(23700.50) Training batch 3000/7151 loss: 0.02731141820549965
(23841.84) Training batch 3500/7151 loss: 0.004899849183857441
(23983.21) Training batch 4000/7151 loss: 0.06308483332395554
(24124.63) Training batch 4500/7151 loss: 0.1740625947713852
(24265.99) Training batch 5000/7151 loss: 0.0019238549284636974
(24407.35) Training batch 5500/7151 loss: 0.001787202199921012
(24548.69) Training batch 6000/7151 loss: 0.05910980701446533
(24690.04) Training batch 6500/7151 loss: 0.0941527783870697
(24831.46) Training batch 7000/7151 loss: 0.045526470988988876
(24873.68) Epoch 10 on training dataset: loss: 3653.590683610644; positive-loss: 3220.92; negative-loss: 432.67 
(24873.68)------------- Training epoch 11 started -------------
(25120.31) Epoch 11 on test dataset: MRR: 0.48; Recalls: tensor([0.3522, 0.6276, 0.7253]) Loss: 54145.06positive-loss: 169.30; negative-loss: 53975.76 
(25120.59) Training batch 0/7151 loss: 0.07502540200948715
(25261.89) Training batch 500/7151 loss: 0.0006149299442768097
(25403.26) Training batch 1000/7151 loss: 0.2155834287405014
(25544.67) Training batch 1500/7151 loss: 0.0023198642302304506
(25686.05) Training batch 2000/7151 loss: 0.08678961545228958
(25827.48) Training batch 2500/7151 loss: 0.3201577663421631
(25968.88) Training batch 3000/7151 loss: 0.01048823818564415
(26110.30) Training batch 3500/7151 loss: 0.0006620890344493091
(26251.73) Training batch 4000/7151 loss: 0.11463645845651627
(26393.13) Training batch 4500/7151 loss: 0.0007588286534883082
(26534.55) Training batch 5000/7151 loss: 0.016143223270773888
(26675.96) Training batch 5500/7151 loss: 0.014217018149793148
(26817.39) Training batch 6000/7151 loss: 0.003883782774209976
(26958.81) Training batch 6500/7151 loss: 0.044914357364177704
(27100.24) Training batch 7000/7151 loss: 0.008408593013882637
(27142.53) Epoch 11 on training dataset: loss: 3568.3837379344222; positive-loss: 3135.25; negative-loss: 433.13 
(27142.53)------------- Training epoch 12 started -------------
(27389.42) Epoch 12 on test dataset: MRR: 0.45; Recalls: tensor([0.3385, 0.5768, 0.6341]) Loss: 61808.26positive-loss: 145.75; negative-loss: 61662.52 
(27389.70) Training batch 0/7151 loss: 0.0626586452126503
(27531.09) Training batch 500/7151 loss: 0.0010195544455200434
(27672.55) Training batch 1000/7151 loss: 0.061690982431173325
(27813.91) Training batch 1500/7151 loss: 0.018380383029580116
(27955.29) Training batch 2000/7151 loss: 0.008369022980332375
(28096.76) Training batch 2500/7151 loss: 0.027979014441370964
(28238.22) Training batch 3000/7151 loss: 0.06281174719333649
(28379.63) Training batch 3500/7151 loss: 0.0063154613599181175
(28521.04) Training batch 4000/7151 loss: 0.018332339823246002
(28662.46) Training batch 4500/7151 loss: 0.0013058027252554893
(28803.91) Training batch 5000/7151 loss: 0.002302099484950304
(28945.32) Training batch 5500/7151 loss: 0.009697009809315205
(29086.75) Training batch 6000/7151 loss: 0.00018790682952385396
(29228.11) Training batch 6500/7151 loss: 0.0999184399843216
(29369.54) Training batch 7000/7151 loss: 0.02627735584974289
(29411.80) Epoch 12 on training dataset: loss: 3466.4487986356507; positive-loss: 3043.46; negative-loss: 422.99 
(29411.80)------------- Training epoch 13 started -------------
(29658.88) Epoch 13 on test dataset: MRR: 0.44; Recalls: tensor([0.3177, 0.5703, 0.6706]) Loss: 55553.44positive-loss: 128.12; negative-loss: 55425.31 
(29659.16) Training batch 0/7151 loss: 0.07987882196903229
(29800.59) Training batch 500/7151 loss: 0.0005887403385713696
(29942.02) Training batch 1000/7151 loss: 0.11906082183122635
(30083.41) Training batch 1500/7151 loss: 0.008304448798298836
(30224.76) Training batch 2000/7151 loss: 0.023620154708623886
(30366.19) Training batch 2500/7151 loss: 0.007942183874547482
(30507.57) Training batch 3000/7151 loss: 0.006003770045936108
(30648.88) Training batch 3500/7151 loss: 0.001001528580673039
(30790.20) Training batch 4000/7151 loss: 0.26709774136543274
(30931.52) Training batch 4500/7151 loss: 0.0014063200214877725
(31072.91) Training batch 5000/7151 loss: 0.0010824152268469334
(31214.26) Training batch 5500/7151 loss: 0.03518235310912132
(31355.62) Training batch 6000/7151 loss: 0.0014340002089738846
(31497.00) Training batch 6500/7151 loss: 0.24125640094280243
(31638.44) Training batch 7000/7151 loss: 0.10789290815591812
(31680.70) Epoch 13 on training dataset: loss: 3559.1639128378574; positive-loss: 3146.38; negative-loss: 412.78 
(31680.70)------------- Training epoch 14 started -------------
(31927.30) Epoch 14 on test dataset: MRR: 0.45; Recalls: tensor([0.3223, 0.6016, 0.7103]) Loss: 50267.81positive-loss: 126.12; negative-loss: 50141.69 
(31927.59) Training batch 0/7151 loss: 0.057238779962062836
(32068.92) Training batch 500/7151 loss: 0.0030421926639974117
(32210.35) Training batch 1000/7151 loss: 0.09577935934066772
(32351.75) Training batch 1500/7151 loss: 0.005800327751785517
(32493.08) Training batch 2000/7151 loss: 0.16134725511074066
(32634.53) Training batch 2500/7151 loss: 0.203708216547966
(32775.97) Training batch 3000/7151 loss: 0.0733243077993393
(32917.39) Training batch 3500/7151 loss: 0.06259966641664505
(33058.83) Training batch 4000/7151 loss: 0.03695741668343544
(33200.26) Training batch 4500/7151 loss: 0.001495253760367632
(33341.75) Training batch 5000/7151 loss: 0.0010461375350132585
(33483.20) Training batch 5500/7151 loss: 0.006455575581640005
(33624.63) Training batch 6000/7151 loss: 0.0009325678693130612
(33766.04) Training batch 6500/7151 loss: 0.39729419350624084
(33907.49) Training batch 7000/7151 loss: 0.00937374122440815
(33949.76) Epoch 14 on training dataset: loss: 3380.587916933826; positive-loss: 2979.61; negative-loss: 400.98 
(33949.76)------------- Training epoch 15 started -------------
(34196.73) Epoch 15 on test dataset: MRR: 0.44; Recalls: tensor([0.3268, 0.5788, 0.6302]) Loss: 59482.93positive-loss: 159.68; negative-loss: 59323.25 
(34197.01) Training batch 0/7151 loss: 0.0182751826941967
(34338.38) Training batch 500/7151 loss: 0.018572097644209862
(34479.79) Training batch 1000/7151 loss: 0.12165090441703796
(34621.23) Training batch 1500/7151 loss: 0.009572112001478672
(34762.67) Training batch 2000/7151 loss: 0.42420127987861633
(34904.16) Training batch 2500/7151 loss: 0.007968854159116745
(35045.64) Training batch 3000/7151 loss: 0.023409392684698105
(35187.11) Training batch 3500/7151 loss: 0.00400239834561944
(35328.61) Training batch 4000/7151 loss: 0.012468617409467697
(35470.01) Training batch 4500/7151 loss: 0.0005363141535781324
(35611.49) Training batch 5000/7151 loss: 0.0018378599779680371
(35752.94) Training batch 5500/7151 loss: 0.01319954451173544
(35894.37) Training batch 6000/7151 loss: 0.0027220635674893856
(36035.83) Training batch 6500/7151 loss: 0.31351685523986816
(36177.32) Training batch 7000/7151 loss: 0.08561676740646362
(36219.60) Epoch 15 on training dataset: loss: 3447.3687039110264; positive-loss: 3047.91; negative-loss: 399.46 
(36219.60)------------- Training epoch 16 started -------------
(36466.55) Epoch 16 on test dataset: MRR: 0.47; Recalls: tensor([0.3366, 0.6178, 0.7018]) Loss: 44690.09positive-loss: 207.91; negative-loss: 44482.18 
(36466.84) Training batch 0/7151 loss: 0.03426629304885864
(36608.22) Training batch 500/7151 loss: 0.0033082040026783943
(36749.71) Training batch 1000/7151 loss: 0.03703891858458519
(36891.17) Training batch 1500/7151 loss: 0.000871051219291985
(37032.64) Training batch 2000/7151 loss: 0.14844197034835815
(37174.13) Training batch 2500/7151 loss: 0.167563334107399
(37315.66) Training batch 3000/7151 loss: 0.2747242748737335
(37457.09) Training batch 3500/7151 loss: 0.01110301073640585
(37598.57) Training batch 4000/7151 loss: 0.023940689861774445
(37740.04) Training batch 4500/7151 loss: 0.0012124916538596153
(37881.55) Training batch 5000/7151 loss: 0.014714017510414124
(38023.04) Training batch 5500/7151 loss: 0.0014877733774483204
(38164.52) Training batch 6000/7151 loss: 0.0022909599356353283
(38306.00) Training batch 6500/7151 loss: 0.014016148634254932
(38447.54) Training batch 7000/7151 loss: 0.04883413016796112
(38489.82) Epoch 16 on training dataset: loss: 3292.353801387333; positive-loss: 2896.86; negative-loss: 395.49 
(38489.82)------------- Training epoch 17 started -------------
(38737.54) Epoch 17 on test dataset: MRR: 0.44; Recalls: tensor([0.3236, 0.5749, 0.6276]) Loss: 57425.58positive-loss: 163.20; negative-loss: 57262.38 
(38737.83) Training batch 0/7151 loss: 0.19021564722061157
(38879.29) Training batch 500/7151 loss: 0.01510162465274334
(39020.77) Training batch 1000/7151 loss: 0.16389259696006775
(39162.23) Training batch 1500/7151 loss: 0.010056180879473686
(39303.71) Training batch 2000/7151 loss: 0.015715615823864937
(39445.21) Training batch 2500/7151 loss: 0.01818828471004963
(39586.70) Training batch 3000/7151 loss: 0.19531674683094025
(39728.15) Training batch 3500/7151 loss: 0.0038234388921409845
(39869.67) Training batch 4000/7151 loss: 0.043709646910429
(40011.20) Training batch 4500/7151 loss: 0.006596334278583527
(40152.72) Training batch 5000/7151 loss: 0.0012325870338827372
(40294.25) Training batch 5500/7151 loss: 0.0031920489855110645
(40435.79) Training batch 6000/7151 loss: 0.008927736431360245
(40577.29) Training batch 6500/7151 loss: 0.06366907060146332
(40718.88) Training batch 7000/7151 loss: 0.022645914927124977
(40761.16) Epoch 17 on training dataset: loss: 3545.310077290742; positive-loss: 3144.73; negative-loss: 400.58 
(40761.16)------------- Training epoch 18 started -------------
(41008.06) Epoch 18 on test dataset: MRR: 0.47; Recalls: tensor([0.3451, 0.5964, 0.6875]) Loss: 54811.90positive-loss: 128.42; negative-loss: 54683.48 
(41008.34) Training batch 0/7151 loss: 0.053027085959911346
(41149.77) Training batch 500/7151 loss: 0.008510132320225239
(41291.27) Training batch 1000/7151 loss: 0.7229198813438416
(41432.84) Training batch 1500/7151 loss: 0.0357147715985775
(41574.41) Training batch 2000/7151 loss: 0.00445373123511672
(41716.06) Training batch 2500/7151 loss: 0.024116050451993942
(41857.58) Training batch 3000/7151 loss: 0.11537177860736847
(41999.09) Training batch 3500/7151 loss: 0.0022754878737032413
(42140.63) Training batch 4000/7151 loss: 0.02152802050113678
(42282.19) Training batch 4500/7151 loss: 0.09267832338809967
(42423.87) Training batch 5000/7151 loss: 0.0006467122584581375
(42565.94) Training batch 5500/7151 loss: 0.008962131105363369
(42707.64) Training batch 6000/7151 loss: 0.061112016439437866
(42849.19) Training batch 6500/7151 loss: 0.048420555889606476
(42990.74) Training batch 7000/7151 loss: 0.01451956294476986
(43033.04) Epoch 18 on training dataset: loss: 3264.5844354014625; positive-loss: 2871.68; negative-loss: 392.91 
(43033.04)------------- Training epoch 19 started -------------
(43279.83) Epoch 19 on test dataset: MRR: 0.44; Recalls: tensor([0.3158, 0.6009, 0.6654]) Loss: 57858.99positive-loss: 178.67; negative-loss: 57680.32 
(43280.12) Training batch 0/7151 loss: 0.026647642254829407
(43421.61) Training batch 500/7151 loss: 0.005775068886578083
(43563.11) Training batch 1000/7151 loss: 0.09552333503961563
(43704.59) Training batch 1500/7151 loss: 0.004656547214835882
(43846.09) Training batch 2000/7151 loss: 0.11554194241762161
(43987.63) Training batch 2500/7151 loss: 0.1866973638534546
(44129.10) Training batch 3000/7151 loss: 0.11818456649780273
(44270.58) Training batch 3500/7151 loss: 0.005005825776606798
(44412.04) Training batch 4000/7151 loss: 0.05694142356514931
(44553.49) Training batch 4500/7151 loss: 0.00047720683505758643
(44694.94) Training batch 5000/7151 loss: 0.04912107437849045
(44836.33) Training batch 5500/7151 loss: 0.008525419980287552
(44977.72) Training batch 6000/7151 loss: 0.0018485505133867264
(45119.14) Training batch 6500/7151 loss: 0.17873145639896393
(45260.58) Training batch 7000/7151 loss: 0.02906208485364914
(45302.84) Epoch 19 on training dataset: loss: 3389.4842576854026; positive-loss: 2992.55; negative-loss: 396.93 
(45302.84)------------- Training epoch 20 started -------------
(45549.35) Epoch 20 on test dataset: MRR: 0.46; Recalls: tensor([0.3262, 0.6159, 0.7122]) Loss: 48394.81positive-loss: 191.26; negative-loss: 48203.56 
(45549.64) Training batch 0/7151 loss: 0.0290501918643713
(45690.97) Training batch 500/7151 loss: 0.0022268465254455805
(45832.41) Training batch 1000/7151 loss: 0.06735576689243317
(45973.85) Training batch 1500/7151 loss: 0.0002608046925161034
(46115.29) Training batch 2000/7151 loss: 0.029853319749236107
(46256.76) Training batch 2500/7151 loss: 0.046885158866643906
(46398.22) Training batch 3000/7151 loss: 0.0025282844435423613
(46539.63) Training batch 3500/7151 loss: 0.01150980219244957
(46681.02) Training batch 4000/7151 loss: 0.018348854035139084
(46822.44) Training batch 4500/7151 loss: 0.0010569595033302903
(46963.86) Training batch 5000/7151 loss: 0.001155761070549488
(47105.22) Training batch 5500/7151 loss: 0.002021435648202896
(47246.66) Training batch 6000/7151 loss: 0.0012799905380234122
(47388.07) Training batch 6500/7151 loss: 0.1934705674648285
(47529.50) Training batch 7000/7151 loss: 0.019880175590515137
(47571.75) Epoch 20 on training dataset: loss: 3179.480622790805; positive-loss: 2816.81; negative-loss: 362.67 
(47571.75)------------- Training epoch 21 started -------------
(47818.31) Epoch 21 on test dataset: MRR: 0.46; Recalls: tensor([0.3294, 0.6113, 0.7135]) Loss: 47247.70positive-loss: 179.41; negative-loss: 47068.29 
(47818.59) Training batch 0/7151 loss: 0.06933967024087906
(47960.03) Training batch 500/7151 loss: 0.02621024288237095
(48101.52) Training batch 1000/7151 loss: 0.11980593204498291
(48242.95) Training batch 1500/7151 loss: 0.00047497323248535395
(48384.38) Training batch 2000/7151 loss: 0.07523485273122787
(48525.84) Training batch 2500/7151 loss: 0.05650368705391884
(48667.26) Training batch 3000/7151 loss: 0.015525996685028076
(48808.74) Training batch 3500/7151 loss: 0.0007364617777056992
(48950.26) Training batch 4000/7151 loss: 0.013127907179296017
(49091.65) Training batch 4500/7151 loss: 0.0046892063692212105
(49233.04) Training batch 5000/7151 loss: 0.002265490125864744
(49374.53) Training batch 5500/7151 loss: 0.0061239576898515224
(49515.99) Training batch 6000/7151 loss: 0.0035540044773370028
(49657.41) Training batch 6500/7151 loss: 0.04859354719519615
(49798.82) Training batch 7000/7151 loss: 0.07226726412773132
(49841.10) Epoch 21 on training dataset: loss: 3276.055907546608; positive-loss: 2879.34; negative-loss: 396.72 
(49841.10)------------- Training epoch 22 started -------------
(50087.66) Epoch 22 on test dataset: MRR: 0.46; Recalls: tensor([0.3223, 0.6074, 0.6999]) Loss: 49435.93positive-loss: 269.17; negative-loss: 49166.75 
(50087.95) Training batch 0/7151 loss: 0.385721355676651
(50229.32) Training batch 500/7151 loss: 0.001961981412023306
(50370.79) Training batch 1000/7151 loss: 0.10681167989969254
(50512.22) Training batch 1500/7151 loss: 0.0008711576811037958
(50653.64) Training batch 2000/7151 loss: 0.14998579025268555
(50795.05) Training batch 2500/7151 loss: 0.0799729973077774
(50936.40) Training batch 3000/7151 loss: 0.0033181270118802786
(51077.81) Training batch 3500/7151 loss: 0.0017189348582178354
(51219.28) Training batch 4000/7151 loss: 0.016059737652540207
(51360.63) Training batch 4500/7151 loss: 0.0017915192293003201
(51502.09) Training batch 5000/7151 loss: 0.0009864362655207515
(51643.52) Training batch 5500/7151 loss: 0.04089462757110596
(51784.98) Training batch 6000/7151 loss: 0.0005558636621572077
(51926.43) Training batch 6500/7151 loss: 0.6207665205001831
(52067.91) Training batch 7000/7151 loss: 0.03970560431480408
(52110.22) Epoch 22 on training dataset: loss: 3343.8176813973478; positive-loss: 2969.36; negative-loss: 374.46 
(52110.22)------------- Training epoch 23 started -------------
(52356.88) Epoch 23 on test dataset: MRR: 0.47; Recalls: tensor([0.3424, 0.6035, 0.6842]) Loss: 47503.71positive-loss: 224.65; negative-loss: 47279.05 
(52357.17) Training batch 0/7151 loss: 0.014348773285746574
(52498.64) Training batch 500/7151 loss: 0.002975202165544033
(52640.09) Training batch 1000/7151 loss: 0.020812343806028366
(52781.56) Training batch 1500/7151 loss: 0.0013689228799194098
(52923.04) Training batch 2000/7151 loss: 0.09433571994304657
(53064.49) Training batch 2500/7151 loss: 0.10029830038547516
(53205.93) Training batch 3000/7151 loss: 0.002446236787363887
(53347.34) Training batch 3500/7151 loss: 0.0008030234603211284
(53488.74) Training batch 4000/7151 loss: 0.11994796246290207
(53630.15) Training batch 4500/7151 loss: 0.002571775345131755
(53771.52) Training batch 5000/7151 loss: 0.0005663125193677843
(53913.00) Training batch 5500/7151 loss: 0.009906665422022343
(54054.45) Training batch 6000/7151 loss: 0.0003883864847011864
(54195.88) Training batch 6500/7151 loss: 0.04306257888674736
(54337.35) Training batch 7000/7151 loss: 0.031389474868774414
(54379.62) Epoch 23 on training dataset: loss: 3126.3538493929445; positive-loss: 2755.51; negative-loss: 370.84 
(54379.62)------------- Training epoch 24 started -------------
(54625.95) Epoch 24 on test dataset: MRR: 0.47; Recalls: tensor([0.3424, 0.6250, 0.7214]) Loss: 50038.34positive-loss: 173.22; negative-loss: 49865.13 
(54626.23) Training batch 0/7151 loss: 0.056100182235240936
(54767.61) Training batch 500/7151 loss: 0.0007250944618135691
(54909.08) Training batch 1000/7151 loss: 0.03426046296954155
(55050.53) Training batch 1500/7151 loss: 0.000430981075624004
(55191.98) Training batch 2000/7151 loss: 0.006501634139567614
(55333.47) Training batch 2500/7151 loss: 0.3402136564254761
(55474.95) Training batch 3000/7151 loss: 0.005684722680598497
(55616.42) Training batch 3500/7151 loss: 0.006785279139876366
(55757.90) Training batch 4000/7151 loss: 0.22348357737064362
(55899.42) Training batch 4500/7151 loss: 0.05147598683834076
(56040.97) Training batch 5000/7151 loss: 0.003760294755920768
(56182.58) Training batch 5500/7151 loss: 0.0017339392798021436
(56324.15) Training batch 6000/7151 loss: 0.001395540777593851
(56465.73) Training batch 6500/7151 loss: 0.03880467638373375
(56607.29) Training batch 7000/7151 loss: 0.09237547963857651
(56649.59) Epoch 24 on training dataset: loss: 3129.901325647181; positive-loss: 2744.89; negative-loss: 385.01 
(56649.59)------------- Training epoch 25 started -------------
(56896.31) Epoch 25 on test dataset: MRR: 0.48; Recalls: tensor([0.3431, 0.6204, 0.7246]) Loss: 42998.47positive-loss: 252.47; negative-loss: 42745.99 
(56896.59) Training batch 0/7151 loss: 0.15205083787441254
(57038.02) Training batch 500/7151 loss: 0.000857221835758537
(57179.54) Training batch 1000/7151 loss: 0.01860942877829075
(57320.97) Training batch 1500/7151 loss: 0.0005380790098570287
(57462.49) Training batch 2000/7151 loss: 0.07728274166584015
(57604.02) Training batch 2500/7151 loss: 0.05379738286137581
(57745.48) Training batch 3000/7151 loss: 0.0038349179085344076
(57886.94) Training batch 3500/7151 loss: 0.009526137262582779
(58028.42) Training batch 4000/7151 loss: 0.2300223857164383
(58169.94) Training batch 4500/7151 loss: 0.006481056101620197
(58311.49) Training batch 5000/7151 loss: 0.0042660473845899105
(58453.03) Training batch 5500/7151 loss: 0.0009763742564246058
(58594.57) Training batch 6000/7151 loss: 0.00036956730764359236
(58736.11) Training batch 6500/7151 loss: 0.009853526949882507
(58877.69) Training batch 7000/7151 loss: 0.026167938485741615
(58919.98) Epoch 25 on training dataset: loss: 3115.0036153181136; positive-loss: 2755.19; negative-loss: 359.82 
(58919.98)------------- Training epoch 26 started -------------
(59166.39) Epoch 26 on test dataset: MRR: 0.47; Recalls: tensor([0.3509, 0.6042, 0.7122]) Loss: 40793.91positive-loss: 258.45; negative-loss: 40535.45 
(59166.68) Training batch 0/7151 loss: 0.012119258753955364
(59308.14) Training batch 500/7151 loss: 0.022504055872559547
(59449.73) Training batch 1000/7151 loss: 0.051634859293699265
(59591.29) Training batch 1500/7151 loss: 0.0555274523794651
(59732.84) Training batch 2000/7151 loss: 0.004767607431858778
(59874.41) Training batch 2500/7151 loss: 0.42254453897476196
(60016.00) Training batch 3000/7151 loss: 0.010429736226797104
(60157.59) Training batch 3500/7151 loss: 0.002315836725756526
(60299.18) Training batch 4000/7151 loss: 0.06034383177757263
(60440.74) Training batch 4500/7151 loss: 0.0769757404923439
(60582.28) Training batch 5000/7151 loss: 0.0034940873738378286
(60723.84) Training batch 5500/7151 loss: 0.0028242061380296946
(60865.38) Training batch 6000/7151 loss: 0.0017937460215762258
(61006.70) Training batch 6500/7151 loss: 0.025372257456183434
(61147.93) Training batch 7000/7151 loss: 0.01021181046962738
(61190.13) Epoch 26 on training dataset: loss: 3181.2248391815374; positive-loss: 2814.94; negative-loss: 366.29 
(61190.13)------------- Training epoch 27 started -------------
(61436.16) Epoch 27 on test dataset: MRR: 0.46; Recalls: tensor([0.3275, 0.6217, 0.7116]) Loss: 43150.63positive-loss: 201.98; negative-loss: 42948.64 
(61436.44) Training batch 0/7151 loss: 0.02203206531703472
(61577.42) Training batch 500/7151 loss: 0.0011536163510754704
(61718.41) Training batch 1000/7151 loss: 1.2252835035324097
(61859.33) Training batch 1500/7151 loss: 0.013698605820536613
(62000.22) Training batch 2000/7151 loss: 0.09679662436246872
(62141.13) Training batch 2500/7151 loss: 0.011153200641274452
(62281.96) Training batch 3000/7151 loss: 0.08778942376375198
(62422.79) Training batch 3500/7151 loss: 0.011487983167171478
(62563.61) Training batch 4000/7151 loss: 0.05145317316055298
(62704.37) Training batch 4500/7151 loss: 0.0009074933477677405
(62845.12) Training batch 5000/7151 loss: 0.04759052395820618
(62985.82) Training batch 5500/7151 loss: 0.0007924540550448
(63126.54) Training batch 6000/7151 loss: 0.0017333977157250047
(63267.20) Training batch 6500/7151 loss: 1.214866280555725
(63407.94) Training batch 7000/7151 loss: 0.10050420463085175
(63449.97) Epoch 27 on training dataset: loss: 3032.0107465578185; positive-loss: 2678.26; negative-loss: 353.75 
(63449.97)------------- Training epoch 28 started -------------
(63695.21) Epoch 28 on test dataset: MRR: 0.45; Recalls: tensor([0.3099, 0.6100, 0.7142]) Loss: 39967.49positive-loss: 415.00; negative-loss: 39552.49 
(63695.50) Training batch 0/7151 loss: 0.019509775564074516
(63836.18) Training batch 500/7151 loss: 0.0007752152159810066
(63976.85) Training batch 1000/7151 loss: 0.19522249698638916
(64117.47) Training batch 1500/7151 loss: 0.013338069431483746
(64258.06) Training batch 2000/7151 loss: 0.0022222530096769333
(64398.66) Training batch 2500/7151 loss: 0.14163696765899658
(64539.19) Training batch 3000/7151 loss: 0.00418895622715354
(64679.76) Training batch 3500/7151 loss: 0.0012121156323701143
(64820.33) Training batch 4000/7151 loss: 0.09708616137504578
(64960.86) Training batch 4500/7151 loss: 0.09585465490818024
(65101.44) Training batch 5000/7151 loss: 0.0019045653752982616
(65241.99) Training batch 5500/7151 loss: 0.0019349822541698813
(65382.57) Training batch 6000/7151 loss: 0.0004672559443861246
(65523.13) Training batch 6500/7151 loss: 0.09711630642414093
(65663.73) Training batch 7000/7151 loss: 0.04767518490552902
(65705.75) Epoch 28 on training dataset: loss: 3144.3519559106644; positive-loss: 2777.21; negative-loss: 367.14 
(65705.75)------------- Training epoch 29 started -------------
(65950.55) Epoch 29 on test dataset: MRR: 0.45; Recalls: tensor([0.3281, 0.5885, 0.6875]) Loss: 51813.54positive-loss: 159.70; negative-loss: 51653.84 
(65950.83) Training batch 0/7151 loss: 0.038192570209503174
(66091.33) Training batch 500/7151 loss: 0.010894074104726315
(66231.83) Training batch 1000/7151 loss: 0.6407181024551392
(66372.29) Training batch 1500/7151 loss: 0.0022297189570963383
(66512.77) Training batch 2000/7151 loss: 0.0430513396859169
(66653.29) Training batch 2500/7151 loss: 0.03301108255982399
(66793.79) Training batch 3000/7151 loss: 0.020740529522299767
(66934.31) Training batch 3500/7151 loss: 0.0014637449057772756
(67074.80) Training batch 4000/7151 loss: 0.06730718165636063
(67215.29) Training batch 4500/7151 loss: 0.00039828632725402713
(67355.78) Training batch 5000/7151 loss: 0.002469947561621666
(67496.24) Training batch 5500/7151 loss: 0.010968687944114208
(67636.67) Training batch 6000/7151 loss: 0.006381593178957701
(67777.06) Training batch 6500/7151 loss: 0.11977485567331314
(67917.48) Training batch 7000/7151 loss: 0.0696093812584877
(67959.45) Epoch 29 on training dataset: loss: 3164.2693375494127; positive-loss: 2799.36; negative-loss: 364.91 
Model saved to cross_encoder.pt
(69392.15) Final test on test dataset!MRR: 0.46; Recalls: tensor([0.3451, 0.5846, 0.6628]) Loss: 43618.00positive-loss: 76.13; negative-loss: 43541.87 
