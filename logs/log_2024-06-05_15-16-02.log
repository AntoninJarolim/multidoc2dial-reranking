Current time: 2024-06-05_15-16-02
train: True
num_epochs: 30
stop_time: None
load_model_path: None
save_model_path: cross_encoder.pt
bert_model_name: cross-encoder/ms-marco-MiniLM-L-6-v2
train_data_path: data/DPR_pairs/DPR_pairs_train_50-60.json
lr: 1e-05
weight_decay: 0.01
dropout_rate: 0.1
label_smoothing: 0
gradient_clip: 0
compute_recall_at_k: False
(456.16)------------- Training epoch 0 started -------------
(578.35) Epoch 0 on test dataset: MRR: 0.25; Recalls: tensor([0.1236, 0.3752, 0.5642]) Loss: 7776.03positive-loss: 2858.16; negative-loss: 4917.86 
(578.54) Training batch 0/7151 loss: 0.7698595523834229
(596.86) Training batch 500/7151 loss: 0.1008295938372612
(615.30) Training batch 1000/7151 loss: 0.1392991691827774
(633.77) Training batch 1500/7151 loss: 0.11037104576826096
(652.34) Training batch 2000/7151 loss: 0.22918801009655
(670.86) Training batch 2500/7151 loss: 0.279275506734848
(689.43) Training batch 3000/7151 loss: 0.12033320963382721
(708.03) Training batch 3500/7151 loss: 0.1027408167719841
(726.61) Training batch 4000/7151 loss: 0.2715950310230255
(745.19) Training batch 4500/7151 loss: 0.1604141741991043
(763.80) Training batch 5000/7151 loss: 0.22531794011592865
(782.41) Training batch 5500/7151 loss: 0.19603027403354645
(801.07) Training batch 6000/7151 loss: 0.03932590410113335
(819.70) Training batch 6500/7151 loss: 0.36069831252098083
(838.29) Training batch 7000/7151 loss: 0.0945618599653244
(843.88) Epoch 0 on training dataset: loss: 12107.45765183121; positive-loss: 11394.07; negative-loss: 713.38 
(843.88)------------- Training epoch 1 started -------------
(965.66) Epoch 1 on test dataset: MRR: 0.55; Recalls: tensor([0.4145, 0.7137, 0.7956]) Loss: 6804.85positive-loss: 1260.96; negative-loss: 5543.89 
(965.70) Training batch 0/7151 loss: 0.6021918654441833
(984.02) Training batch 500/7151 loss: 0.01074009295552969
(1002.43) Training batch 1000/7151 loss: 0.15051822364330292
(1020.90) Training batch 1500/7151 loss: 0.061522990465164185
(1039.43) Training batch 2000/7151 loss: 0.2188468873500824
(1058.08) Training batch 2500/7151 loss: 0.24809907376766205
(1076.74) Training batch 3000/7151 loss: 0.11778618395328522
(1094.88) Training batch 3500/7151 loss: 0.04539598897099495
(1113.04) Training batch 4000/7151 loss: 0.13351377844810486
(1131.21) Training batch 4500/7151 loss: 0.08461958169937134
(1149.39) Training batch 5000/7151 loss: 0.05811988562345505
(1167.57) Training batch 5500/7151 loss: 0.19538214802742004
(1185.75) Training batch 6000/7151 loss: 0.10700558125972748
(1203.93) Training batch 6500/7151 loss: 0.29928678274154663
(1222.11) Training batch 7000/7151 loss: 0.07091328501701355
(1227.55) Epoch 1 on training dataset: loss: 8880.7628441602; positive-loss: 8347.81; negative-loss: 532.96 
(1227.55)------------- Training epoch 2 started -------------
(1342.39) Epoch 2 on test dataset: MRR: 0.55; Recalls: tensor([0.4165, 0.7106, 0.7948]) Loss: 7483.33positive-loss: 1214.90; negative-loss: 6268.43 
(1342.43) Training batch 0/7151 loss: 0.5829269289970398
(1360.25) Training batch 500/7151 loss: 0.006665260065346956
(1378.19) Training batch 1000/7151 loss: 0.16750891506671906
(1396.23) Training batch 1500/7151 loss: 0.05617179349064827
(1414.29) Training batch 2000/7151 loss: 0.20002220571041107
(1432.40) Training batch 2500/7151 loss: 0.18560069799423218
(1450.53) Training batch 3000/7151 loss: 0.06943004578351974
(1468.67) Training batch 3500/7151 loss: 0.01636774092912674
(1486.82) Training batch 4000/7151 loss: 0.23970575630664825
(1504.98) Training batch 4500/7151 loss: 0.16012415289878845
(1523.14) Training batch 5000/7151 loss: 0.014832895249128342
(1541.30) Training batch 5500/7151 loss: 0.10865256935358047
(1559.45) Training batch 6000/7151 loss: 0.049825940281152725
(1577.61) Training batch 6500/7151 loss: 0.20464758574962616
(1595.76) Training batch 7000/7151 loss: 0.019399957731366158
(1601.19) Epoch 2 on training dataset: loss: 7367.045006599743; positive-loss: 6913.09; negative-loss: 453.95 
(1601.19)------------- Training epoch 3 started -------------
(1716.44) Epoch 3 on test dataset: MRR: 0.55; Recalls: tensor([0.4231, 0.7096, 0.7936]) Loss: 7997.90positive-loss: 1172.98; negative-loss: 6824.92 
(1716.48) Training batch 0/7151 loss: 0.5181377530097961
(1734.31) Training batch 500/7151 loss: 0.0034983940422534943
(1752.25) Training batch 1000/7151 loss: 0.09105866402387619
(1770.26) Training batch 1500/7151 loss: 0.023678617551922798
(1788.31) Training batch 2000/7151 loss: 0.14634659886360168
(1806.40) Training batch 2500/7151 loss: 0.19637620449066162
(1824.51) Training batch 3000/7151 loss: 0.09177596122026443
(1842.64) Training batch 3500/7151 loss: 0.011242857202887535
(1860.80) Training batch 4000/7151 loss: 0.1210709735751152
(1878.94) Training batch 4500/7151 loss: 0.09185300022363663
(1897.08) Training batch 5000/7151 loss: 0.01917826198041439
(1915.23) Training batch 5500/7151 loss: 0.08761066198348999
(1933.39) Training batch 6000/7151 loss: 0.1593904048204422
(1951.53) Training batch 6500/7151 loss: 0.1444193571805954
(1969.68) Training batch 7000/7151 loss: 0.011722683906555176
(1975.11) Epoch 3 on training dataset: loss: 6176.107711123419; positive-loss: 5786.54; negative-loss: 389.57 
(1975.11)------------- Training epoch 4 started -------------
(2090.26) Epoch 4 on test dataset: MRR: 0.54; Recalls: tensor([0.4118, 0.7064, 0.7973]) Loss: 8453.29positive-loss: 1225.16; negative-loss: 7228.13 
(2090.30) Training batch 0/7151 loss: 0.4709930717945099
(2108.13) Training batch 500/7151 loss: 0.0019761149305850267
(2126.06) Training batch 1000/7151 loss: 0.1088244765996933
(2144.07) Training batch 1500/7151 loss: 0.010309064760804176
(2162.13) Training batch 2000/7151 loss: 0.1438354253768921
(2180.21) Training batch 2500/7151 loss: 0.17735721170902252
(2198.30) Training batch 3000/7151 loss: 0.05255940556526184
(2216.40) Training batch 3500/7151 loss: 0.009545890614390373
(2234.51) Training batch 4000/7151 loss: 0.09562910348176956
(2252.67) Training batch 4500/7151 loss: 0.12937414646148682
(2270.79) Training batch 5000/7151 loss: 0.01270989514887333
(2288.91) Training batch 5500/7151 loss: 0.08001559227705002
(2307.03) Training batch 6000/7151 loss: 0.00432470440864563
(2325.15) Training batch 6500/7151 loss: 0.25148680806159973
(2343.28) Training batch 7000/7151 loss: 0.003249269677326083
(2348.70) Epoch 4 on training dataset: loss: 5208.431613549008; positive-loss: 4865.60; negative-loss: 342.84 
(2348.70)------------- Training epoch 5 started -------------
(2463.97) Epoch 5 on test dataset: MRR: 0.55; Recalls: tensor([0.4140, 0.7091, 0.7960]) Loss: 8658.27positive-loss: 1315.29; negative-loss: 7342.98 
(2464.01) Training batch 0/7151 loss: 0.34167999029159546
(2481.83) Training batch 500/7151 loss: 0.0014388429699465632
(2499.76) Training batch 1000/7151 loss: 0.05966506153345108
(2517.77) Training batch 1500/7151 loss: 0.006047809962183237
(2535.83) Training batch 2000/7151 loss: 0.12483292073011398
(2553.91) Training batch 2500/7151 loss: 0.11986956745386124
(2572.01) Training batch 3000/7151 loss: 0.05018044263124466
(2590.13) Training batch 3500/7151 loss: 0.009879673831164837
(2608.24) Training batch 4000/7151 loss: 0.08056934922933578
(2626.36) Training batch 4500/7151 loss: 0.08721436560153961
(2644.48) Training batch 5000/7151 loss: 0.010166198946535587
(2662.61) Training batch 5500/7151 loss: 0.024997500702738762
(2680.73) Training batch 6000/7151 loss: 0.055145081132650375
(2698.85) Training batch 6500/7151 loss: 0.09633003920316696
(2716.98) Training batch 7000/7151 loss: 0.0021574136335402727
(2722.40) Epoch 5 on training dataset: loss: 4362.014652766869; positive-loss: 4064.63; negative-loss: 297.38 
(2722.40)------------- Training epoch 6 started -------------
(2837.33) Epoch 6 on test dataset: MRR: 0.54; Recalls: tensor([0.4057, 0.7074, 0.7907]) Loss: 9202.36positive-loss: 1543.19; negative-loss: 7659.17 
(2837.37) Training batch 0/7151 loss: 0.13462676107883453
(2855.18) Training batch 500/7151 loss: 0.0011536988895386457
(2873.13) Training batch 1000/7151 loss: 0.024192338809370995
(2891.14) Training batch 1500/7151 loss: 0.010792487300932407
(2909.20) Training batch 2000/7151 loss: 0.061477065086364746
(2927.28) Training batch 2500/7151 loss: 0.15247131884098053
(2945.39) Training batch 3000/7151 loss: 0.018542729318141937
(2963.50) Training batch 3500/7151 loss: 0.009190477430820465
(2981.62) Training batch 4000/7151 loss: 0.06582337617874146
(2999.73) Training batch 4500/7151 loss: 0.09283929318189621
(3017.87) Training batch 5000/7151 loss: 0.0021123706828802824
(3036.01) Training batch 5500/7151 loss: 0.01303360890597105
(3054.16) Training batch 6000/7151 loss: 0.08704856038093567
(3072.30) Training batch 6500/7151 loss: 0.03642139583826065
(3090.44) Training batch 7000/7151 loss: 0.0015548387309536338
(3095.87) Epoch 6 on training dataset: loss: 3685.1022456610226; positive-loss: 3426.17; negative-loss: 258.94 
(3095.87)------------- Training epoch 7 started -------------
(3211.16) Epoch 7 on test dataset: MRR: 0.55; Recalls: tensor([0.4123, 0.7069, 0.7880]) Loss: 9212.09positive-loss: 1698.26; negative-loss: 7513.84 
(3211.20) Training batch 0/7151 loss: 0.18817156553268433
(3229.01) Training batch 500/7151 loss: 0.0007980246446095407
(3246.96) Training batch 1000/7151 loss: 0.04879700765013695
(3264.95) Training batch 1500/7151 loss: 0.0017357626929879189
(3282.98) Training batch 2000/7151 loss: 0.03955087438225746
(3301.07) Training batch 2500/7151 loss: 0.08612371236085892
(3319.17) Training batch 3000/7151 loss: 0.01732848770916462
(3337.29) Training batch 3500/7151 loss: 0.0046126218512654305
(3355.40) Training batch 4000/7151 loss: 0.04388558119535446
(3373.53) Training batch 4500/7151 loss: 0.058602478355169296
(3391.66) Training batch 5000/7151 loss: 0.0023042643442749977
(3409.79) Training batch 5500/7151 loss: 0.024896755814552307
(3427.93) Training batch 6000/7151 loss: 0.0012037146370857954
(3446.05) Training batch 6500/7151 loss: 0.05896048620343208
(3464.20) Training batch 7000/7151 loss: 0.0024411557242274284
(3469.63) Epoch 7 on training dataset: loss: 3108.5841035708145; positive-loss: 2878.78; negative-loss: 229.80 
(3469.63)------------- Training epoch 8 started -------------
(3584.74) Epoch 8 on test dataset: MRR: 0.54; Recalls: tensor([0.4094, 0.7059, 0.7838]) Loss: 10771.99positive-loss: 1585.68; negative-loss: 9186.31 
(3584.78) Training batch 0/7151 loss: 0.07289958000183105
(3602.59) Training batch 500/7151 loss: 0.000588292081374675
(3620.53) Training batch 1000/7151 loss: 0.060695577412843704
(3638.53) Training batch 1500/7151 loss: 0.0034234016202390194
(3656.57) Training batch 2000/7151 loss: 0.07175052911043167
(3674.66) Training batch 2500/7151 loss: 0.13221845030784607
(3692.77) Training batch 3000/7151 loss: 0.014657562598586082
(3710.89) Training batch 3500/7151 loss: 0.002717629773542285
(3729.03) Training batch 4000/7151 loss: 0.02516401931643486
(3747.17) Training batch 4500/7151 loss: 0.06421151012182236
(3765.32) Training batch 5000/7151 loss: 0.004784806165844202
(3783.48) Training batch 5500/7151 loss: 0.00432762922719121
(3801.63) Training batch 6000/7151 loss: 0.0005581114091910422
(3819.78) Training batch 6500/7151 loss: 0.16636620461940765
(3837.94) Training batch 7000/7151 loss: 0.0017936580115929246
(3843.36) Epoch 8 on training dataset: loss: 2693.3556947743928; positive-loss: 2491.22; negative-loss: 202.14 
(3843.36)------------- Training epoch 9 started -------------
(3958.39) Epoch 9 on test dataset: MRR: 0.54; Recalls: tensor([0.3989, 0.7025, 0.7843]) Loss: 10828.47positive-loss: 1745.43; negative-loss: 9083.04 
(3958.43) Training batch 0/7151 loss: 0.06768892705440521
(3976.25) Training batch 500/7151 loss: 0.00047196669038385153
(3994.18) Training batch 1000/7151 loss: 0.06396102160215378
(4012.19) Training batch 1500/7151 loss: 0.057863540947437286
(4030.24) Training batch 2000/7151 loss: 0.03193669393658638
(4048.32) Training batch 2500/7151 loss: 0.05140300467610359
(4066.42) Training batch 3000/7151 loss: 0.05427522957324982
(4084.54) Training batch 3500/7151 loss: 0.0317232646048069
(4102.66) Training batch 4000/7151 loss: 0.031878285109996796
(4120.80) Training batch 4500/7151 loss: 0.16475790739059448
(4138.94) Training batch 5000/7151 loss: 0.0006098304875195026
(4157.09) Training batch 5500/7151 loss: 0.0017500892281532288
(4175.23) Training batch 6000/7151 loss: 0.0004572691977955401
(4193.37) Training batch 6500/7151 loss: 0.010831190273165703
(4211.53) Training batch 7000/7151 loss: 0.001388120581395924
(4216.96) Epoch 9 on training dataset: loss: 2309.6160795718606; positive-loss: 2133.76; negative-loss: 175.86 
(4216.96)------------- Training epoch 10 started -------------
(4332.05) Epoch 10 on test dataset: MRR: 0.52; Recalls: tensor([0.3818, 0.6996, 0.7833]) Loss: 12017.69positive-loss: 1709.58; negative-loss: 10308.10 
(4332.09) Training batch 0/7151 loss: 0.12445669621229172
(4349.91) Training batch 500/7151 loss: 0.0004491905274335295
(4367.84) Training batch 1000/7151 loss: 0.0019605644047260284
(4385.85) Training batch 1500/7151 loss: 0.001838610041886568
(4403.90) Training batch 2000/7151 loss: 0.04897214099764824
(4421.99) Training batch 2500/7151 loss: 0.04121162369847298
(4440.10) Training batch 3000/7151 loss: 0.15438632667064667
(4458.21) Training batch 3500/7151 loss: 0.05397780239582062
(4476.33) Training batch 4000/7151 loss: 0.0922013521194458
(4494.47) Training batch 4500/7151 loss: 0.0322706364095211
(4512.62) Training batch 5000/7151 loss: 0.0025282350834459066
(4530.77) Training batch 5500/7151 loss: 0.001356682158075273
(4548.91) Training batch 6000/7151 loss: 0.001958295004442334
(4567.05) Training batch 6500/7151 loss: 0.019049840047955513
(4585.20) Training batch 7000/7151 loss: 0.0011679072631523013
(4590.63) Epoch 10 on training dataset: loss: 1987.7762200451398; positive-loss: 1830.87; negative-loss: 156.91 
(4590.63)------------- Training epoch 11 started -------------
(4705.71) Epoch 11 on test dataset: MRR: 0.53; Recalls: tensor([0.3945, 0.7042, 0.7882]) Loss: 11410.69positive-loss: 1932.79; negative-loss: 9477.91 
(4705.75) Training batch 0/7151 loss: 0.07390742003917694
(4723.57) Training batch 500/7151 loss: 0.0003494144475553185
(4741.49) Training batch 1000/7151 loss: 0.014490633271634579
(4759.50) Training batch 1500/7151 loss: 0.0005047109443694353
(4777.55) Training batch 2000/7151 loss: 0.003978778142482042
(4795.63) Training batch 2500/7151 loss: 0.038596175611019135
(4813.74) Training batch 3000/7151 loss: 0.05117237940430641
(4831.86) Training batch 3500/7151 loss: 0.001563142635859549
(4849.98) Training batch 4000/7151 loss: 0.00444230530411005
(4868.11) Training batch 4500/7151 loss: 0.0076295374892652035
(4886.24) Training batch 5000/7151 loss: 0.0003439645515754819
(4904.39) Training batch 5500/7151 loss: 0.007214168552309275
(4922.53) Training batch 6000/7151 loss: 0.0002945767482742667
(4940.67) Training batch 6500/7151 loss: 0.059051282703876495
(4958.81) Training batch 7000/7151 loss: 0.00035203227889724076
(4964.24) Epoch 11 on training dataset: loss: 1713.6392093479371; positive-loss: 1571.79; negative-loss: 141.85 
(4964.24)------------- Training epoch 12 started -------------
(5079.46) Epoch 12 on test dataset: MRR: 0.52; Recalls: tensor([0.3788, 0.6961, 0.7855]) Loss: 12928.44positive-loss: 1977.03; negative-loss: 10951.42 
(5079.50) Training batch 0/7151 loss: 0.01781506836414337
(5097.32) Training batch 500/7151 loss: 0.00030789084848947823
(5115.24) Training batch 1000/7151 loss: 0.006950249895453453
(5133.24) Training batch 1500/7151 loss: 0.0011391709558665752
(5151.29) Training batch 2000/7151 loss: 0.011536375619471073
(5169.36) Training batch 2500/7151 loss: 0.1838849037885666
(5187.44) Training batch 3000/7151 loss: 0.009908274747431278
(5205.54) Training batch 3500/7151 loss: 0.002279390348121524
(5223.65) Training batch 4000/7151 loss: 0.0374261736869812
(5241.77) Training batch 4500/7151 loss: 0.10845550149679184
(5259.88) Training batch 5000/7151 loss: 0.0002868618175853044
(5278.01) Training batch 5500/7151 loss: 0.009112986735999584
(5296.13) Training batch 6000/7151 loss: 0.00029316998552531004
(5314.25) Training batch 6500/7151 loss: 0.014194491319358349
(5332.38) Training batch 7000/7151 loss: 0.00026602085563354194
(5337.80) Epoch 12 on training dataset: loss: 1520.8871086371364; positive-loss: 1394.58; negative-loss: 126.31 
(5337.80)------------- Training epoch 13 started -------------
(5453.02) Epoch 13 on test dataset: MRR: 0.52; Recalls: tensor([0.3784, 0.6944, 0.7799]) Loss: 14008.38positive-loss: 1763.77; negative-loss: 12244.61 
(5453.06) Training batch 0/7151 loss: 0.026937182992696762
(5470.87) Training batch 500/7151 loss: 0.00038517668144777417
(5488.79) Training batch 1000/7151 loss: 0.0012425623135641217
(5506.80) Training batch 1500/7151 loss: 0.0008635556441731751
(5524.86) Training batch 2000/7151 loss: 0.010535787791013718
(5542.94) Training batch 2500/7151 loss: 0.03953232988715172
(5561.03) Training batch 3000/7151 loss: 0.005377206951379776
(5579.13) Training batch 3500/7151 loss: 0.0007809874950908124
(5597.25) Training batch 4000/7151 loss: 0.09676574915647507
(5615.37) Training batch 4500/7151 loss: 0.054935283958911896
(5633.49) Training batch 5000/7151 loss: 0.00023392897855956107
(5651.60) Training batch 5500/7151 loss: 0.0005930488696321845
(5669.72) Training batch 6000/7151 loss: 0.00027722734375856817
(5687.84) Training batch 6500/7151 loss: 0.05255575478076935
(5705.96) Training batch 7000/7151 loss: 0.00022569562133867294
(5711.38) Epoch 13 on training dataset: loss: 1383.5551176638837; positive-loss: 1267.11; negative-loss: 116.44 
(5711.38)------------- Training epoch 14 started -------------
(5826.50) Epoch 14 on test dataset: MRR: 0.52; Recalls: tensor([0.3779, 0.6949, 0.7780]) Loss: 14192.46positive-loss: 1975.28; negative-loss: 12217.18 
(5826.54) Training batch 0/7151 loss: 0.11714139580726624
(5844.34) Training batch 500/7151 loss: 0.00028126765391789377
(5862.25) Training batch 1000/7151 loss: 0.001938097644597292
(5880.26) Training batch 1500/7151 loss: 0.00024652687716297805
(5898.31) Training batch 2000/7151 loss: 0.024673350155353546
(5916.38) Training batch 2500/7151 loss: 0.14395008981227875
(5934.47) Training batch 3000/7151 loss: 0.02836792729794979
(5952.56) Training batch 3500/7151 loss: 0.00024198925530072302
(5970.66) Training batch 4000/7151 loss: 0.026652570813894272
(5988.75) Training batch 4500/7151 loss: 0.013528679497539997
(6006.86) Training batch 5000/7151 loss: 0.0002122974838130176
(6024.96) Training batch 5500/7151 loss: 0.009835555218160152
(6043.08) Training batch 6000/7151 loss: 0.00018984012422151864
(6061.20) Training batch 6500/7151 loss: 0.03252047300338745
(6079.32) Training batch 7000/7151 loss: 0.000558230560272932
(6084.74) Epoch 14 on training dataset: loss: 1200.7918095281784; positive-loss: 1094.94; negative-loss: 105.85 
(6084.74)------------- Training epoch 15 started -------------
(6200.03) Epoch 15 on test dataset: MRR: 0.52; Recalls: tensor([0.3823, 0.6930, 0.7819]) Loss: 14562.11positive-loss: 2010.04; negative-loss: 12552.07 
(6200.06) Training batch 0/7151 loss: 0.004503217060118914
(6217.87) Training batch 500/7151 loss: 0.0001868718391051516
(6235.80) Training batch 1000/7151 loss: 0.0021725355181843042
(6253.80) Training batch 1500/7151 loss: 0.00018205838568974286
(6271.84) Training batch 2000/7151 loss: 0.013370621018111706
(6289.92) Training batch 2500/7151 loss: 0.12037160247564316
(6308.01) Training batch 3000/7151 loss: 0.001440407824702561
(6326.11) Training batch 3500/7151 loss: 0.0014255577698349953
(6344.22) Training batch 4000/7151 loss: 0.0017821879591792822
(6362.33) Training batch 4500/7151 loss: 0.00452154828235507
(6380.45) Training batch 5000/7151 loss: 0.00016041162598412484
(6398.56) Training batch 5500/7151 loss: 0.01546582579612732
(6416.68) Training batch 6000/7151 loss: 0.00018222624203190207
(6434.79) Training batch 6500/7151 loss: 0.04660632088780403
(6452.91) Training batch 7000/7151 loss: 0.02304372936487198
(6458.33) Epoch 15 on training dataset: loss: 1080.3223971666375; positive-loss: 984.82; negative-loss: 95.50 
(6458.33)------------- Training epoch 16 started -------------
(6573.41) Epoch 16 on test dataset: MRR: 0.53; Recalls: tensor([0.3845, 0.6983, 0.7821]) Loss: 15245.02positive-loss: 1722.16; negative-loss: 13522.87 
(6573.45) Training batch 0/7151 loss: 0.0030116415582597256
(6591.27) Training batch 500/7151 loss: 0.0001688245392870158
(6609.20) Training batch 1000/7151 loss: 0.03153834864497185
(6627.20) Training batch 1500/7151 loss: 0.0002147062768926844
(6645.24) Training batch 2000/7151 loss: 0.0017126110615208745
(6663.33) Training batch 2500/7151 loss: 0.07113572955131531
(6681.42) Training batch 3000/7151 loss: 0.0005343083175830543
(6699.52) Training batch 3500/7151 loss: 0.0015691424487158656
(6717.62) Training batch 4000/7151 loss: 0.0020572422072291374
(6735.72) Training batch 4500/7151 loss: 0.0029723201878368855
(6753.84) Training batch 5000/7151 loss: 0.009014835581183434
(6771.95) Training batch 5500/7151 loss: 0.0004305784241296351
(6790.09) Training batch 6000/7151 loss: 0.00014539845869876444
(6808.20) Training batch 6500/7151 loss: 0.058571528643369675
(6826.32) Training batch 7000/7151 loss: 0.00019935524323955178
(6831.74) Epoch 16 on training dataset: loss: 1051.707086230992; positive-loss: 961.54; negative-loss: 90.17 
(6831.74)------------- Training epoch 17 started -------------
(6946.91) Epoch 17 on test dataset: MRR: 0.53; Recalls: tensor([0.3937, 0.7015, 0.7826]) Loss: 14542.45positive-loss: 1876.21; negative-loss: 12666.24 
(6946.95) Training batch 0/7151 loss: 0.12358951568603516
(6964.76) Training batch 500/7151 loss: 0.00015861293650232255
(6982.70) Training batch 1000/7151 loss: 0.0003849053755402565
(7000.70) Training batch 1500/7151 loss: 0.00028633952024392784
(7018.76) Training batch 2000/7151 loss: 0.0029932681936770678
(7036.84) Training batch 2500/7151 loss: 0.00557237584143877
(7054.93) Training batch 3000/7151 loss: 0.0019657574594020844
(7073.03) Training batch 3500/7151 loss: 0.00039877439849078655
(7091.13) Training batch 4000/7151 loss: 0.00018737142090685666
(7109.24) Training batch 4500/7151 loss: 0.054875873029232025
(7127.36) Training batch 5000/7151 loss: 0.00014194408140610904
(7145.50) Training batch 5500/7151 loss: 0.0006617325125262141
(7163.64) Training batch 6000/7151 loss: 0.00013715808745473623
(7181.77) Training batch 6500/7151 loss: 0.006231740582734346
(7199.90) Training batch 7000/7151 loss: 0.00452216574922204
(7205.33) Epoch 17 on training dataset: loss: 953.0025431228278; positive-loss: 873.36; negative-loss: 79.64 
(7205.33)------------- Training epoch 18 started -------------
(7320.51) Epoch 18 on test dataset: MRR: 0.52; Recalls: tensor([0.3830, 0.6996, 0.7782]) Loss: 16192.62positive-loss: 1594.18; negative-loss: 14598.44 
(7320.55) Training batch 0/7151 loss: 0.002184415003284812
(7338.36) Training batch 500/7151 loss: 0.00017404813843313605
(7356.30) Training batch 1000/7151 loss: 0.007208653725683689
(7374.29) Training batch 1500/7151 loss: 0.0034701034892350435
(7392.32) Training batch 2000/7151 loss: 0.018838608637452126
(7410.39) Training batch 2500/7151 loss: 0.007847527973353863
(7428.49) Training batch 3000/7151 loss: 0.01010595727711916
(7446.60) Training batch 3500/7151 loss: 0.00026333556161262095
(7464.72) Training batch 4000/7151 loss: 0.00648467056453228
(7482.83) Training batch 4500/7151 loss: 0.004700225777924061
(7500.96) Training batch 5000/7151 loss: 0.0005638468428514898
(7519.10) Training batch 5500/7151 loss: 0.002068055560812354
(7537.23) Training batch 6000/7151 loss: 0.00012299635272938758
(7555.36) Training batch 6500/7151 loss: 0.04226371645927429
(7573.50) Training batch 7000/7151 loss: 0.00016969538410194218
(7578.92) Epoch 18 on training dataset: loss: 870.7756843069947; positive-loss: 796.47; negative-loss: 74.31 
(7578.92)------------- Training epoch 19 started -------------
(7694.08) Epoch 19 on test dataset: MRR: 0.51; Recalls: tensor([0.3679, 0.6947, 0.7826]) Loss: 14796.05positive-loss: 2101.59; negative-loss: 12694.46 
(7694.12) Training batch 0/7151 loss: 0.001640475238673389
(7711.93) Training batch 500/7151 loss: 0.00014397922495845705
(7729.86) Training batch 1000/7151 loss: 0.0007205228903330863
(7747.86) Training batch 1500/7151 loss: 0.00015478370187338442
(7765.89) Training batch 2000/7151 loss: 0.002855704165995121
(7783.98) Training batch 2500/7151 loss: 0.0030438797548413277
(7802.07) Training batch 3000/7151 loss: 0.0002495288790669292
(7820.18) Training batch 3500/7151 loss: 0.0004280200519133359
(7838.31) Training batch 4000/7151 loss: 0.007503689266741276
(7856.43) Training batch 4500/7151 loss: 0.004001127555966377
(7874.57) Training batch 5000/7151 loss: 0.00012978256563656032
(7892.70) Training batch 5500/7151 loss: 0.006859953515231609
(7910.83) Training batch 6000/7151 loss: 0.00014141664723865688
(7928.97) Training batch 6500/7151 loss: 0.004990403540432453
(7947.11) Training batch 7000/7151 loss: 0.0001262405567103997
(7952.54) Epoch 19 on training dataset: loss: 796.7970233283631; positive-loss: 727.58; negative-loss: 69.22 
(7952.54)------------- Training epoch 20 started -------------
(8067.77) Epoch 20 on test dataset: MRR: 0.52; Recalls: tensor([0.3791, 0.6983, 0.7868]) Loss: 14287.83positive-loss: 2116.25; negative-loss: 12171.58 
(8067.81) Training batch 0/7151 loss: 0.014026804827153683
(8085.61) Training batch 500/7151 loss: 0.00012997264275327325
(8103.53) Training batch 1000/7151 loss: 0.030712410807609558
(8121.54) Training batch 1500/7151 loss: 0.022289197891950607
(8139.60) Training batch 2000/7151 loss: 0.02966727502644062
(8157.69) Training batch 2500/7151 loss: 0.007539060898125172
(8175.80) Training batch 3000/7151 loss: 0.004109577275812626
(8193.92) Training batch 3500/7151 loss: 0.001254000817425549
(8212.06) Training batch 4000/7151 loss: 0.002994663082063198
(8230.21) Training batch 4500/7151 loss: 0.10457894206047058
(8248.36) Training batch 5000/7151 loss: 0.00011612317030085251
(8266.52) Training batch 5500/7151 loss: 0.0008395251352339983
(8284.69) Training batch 6000/7151 loss: 0.00013495121675077826
(8302.84) Training batch 6500/7151 loss: 0.16125290095806122
(8321.00) Training batch 7000/7151 loss: 0.0026286174543201923
(8326.43) Epoch 20 on training dataset: loss: 762.0633345159076; positive-loss: 699.67; negative-loss: 62.39 
(8326.43)------------- Training epoch 21 started -------------
(8441.61) Epoch 21 on test dataset: MRR: 0.52; Recalls: tensor([0.3784, 0.6954, 0.7836]) Loss: 16222.91positive-loss: 1733.63; negative-loss: 14489.28 
(8441.65) Training batch 0/7151 loss: 0.0012343853013589978
(8459.48) Training batch 500/7151 loss: 0.00013628446322400123
(8477.43) Training batch 1000/7151 loss: 0.0005229634698480368
(8495.46) Training batch 1500/7151 loss: 0.00012169871479272842
(8513.53) Training batch 2000/7151 loss: 0.0017189066857099533
(8531.62) Training batch 2500/7151 loss: 0.0078111616894602776
(8549.74) Training batch 3000/7151 loss: 0.018038449808955193
(8567.88) Training batch 3500/7151 loss: 0.00015623496437910944
(8586.04) Training batch 4000/7151 loss: 0.0010792125249281526
(8604.20) Training batch 4500/7151 loss: 0.00022516241006087512
(8622.35) Training batch 5000/7151 loss: 0.00014825791004113853
(8640.52) Training batch 5500/7151 loss: 0.005200517363846302
(8658.68) Training batch 6000/7151 loss: 0.00019335445540491492
(8676.84) Training batch 6500/7151 loss: 0.019370971247553825
(8694.99) Training batch 7000/7151 loss: 0.00013345071056392044
(8700.43) Epoch 21 on training dataset: loss: 693.8812425439974; positive-loss: 633.51; negative-loss: 60.37 
(8700.43)------------- Training epoch 22 started -------------
(8815.64) Epoch 22 on test dataset: MRR: 0.51; Recalls: tensor([0.3561, 0.7037, 0.7892]) Loss: 13798.99positive-loss: 2609.22; negative-loss: 11189.77 
(8815.68) Training batch 0/7151 loss: 0.03591502830386162
(8833.52) Training batch 500/7151 loss: 0.0001222886348841712
(8851.47) Training batch 1000/7151 loss: 0.0008809666614979506
(8869.51) Training batch 1500/7151 loss: 0.00011311932757962495
(8887.59) Training batch 2000/7151 loss: 0.008767932653427124
(8905.71) Training batch 2500/7151 loss: 0.005525644402951002
(8923.84) Training batch 3000/7151 loss: 0.0012743936385959387
(8941.98) Training batch 3500/7151 loss: 0.00015989209350664169
(8960.14) Training batch 4000/7151 loss: 0.00019551953300833702
(8978.29) Training batch 4500/7151 loss: 0.015332009643316269
(8996.45) Training batch 5000/7151 loss: 9.995009168051183e-05
(9014.62) Training batch 5500/7151 loss: 0.0018616745946928859
(9032.79) Training batch 6000/7151 loss: 0.0002410909510217607
(9050.96) Training batch 6500/7151 loss: 0.0026746606454253197
(9069.14) Training batch 7000/7151 loss: 0.00019016038277186453
(9074.58) Epoch 22 on training dataset: loss: 659.6529083574715; positive-loss: 602.61; negative-loss: 57.05 
(9074.58)------------- Training epoch 23 started -------------
(9189.84) Epoch 23 on test dataset: MRR: 0.50; Recalls: tensor([0.3437, 0.6991, 0.7865]) Loss: 13730.93positive-loss: 2773.14; negative-loss: 10957.79 
(9189.88) Training batch 0/7151 loss: 0.01479153148829937
(9207.71) Training batch 500/7151 loss: 0.00010908534022746608
(9225.68) Training batch 1000/7151 loss: 0.0002773346204776317
(9243.70) Training batch 1500/7151 loss: 0.00010549423313932493
(9261.77) Training batch 2000/7151 loss: 0.08030080050230026
(9279.89) Training batch 2500/7151 loss: 0.03236924484372139
(9298.04) Training batch 3000/7151 loss: 0.0033996948041021824
(9316.19) Training batch 3500/7151 loss: 0.00031877774745225906
(9334.36) Training batch 4000/7151 loss: 0.0035577542148530483
(9352.52) Training batch 4500/7151 loss: 0.00038155613583512604
(9370.69) Training batch 5000/7151 loss: 8.990143396658823e-05
(9388.86) Training batch 5500/7151 loss: 0.002575089456513524
(9407.03) Training batch 6000/7151 loss: 0.00011408758291509002
(9425.19) Training batch 6500/7151 loss: 0.0009241125080734491
(9443.37) Training batch 7000/7151 loss: 9.379811672260985e-05
(9448.80) Epoch 23 on training dataset: loss: 604.3760060327149; positive-loss: 554.31; negative-loss: 50.06 
(9448.80)------------- Training epoch 24 started -------------
(9563.77) Epoch 24 on test dataset: MRR: 0.51; Recalls: tensor([0.3625, 0.7003, 0.7824]) Loss: 15216.20positive-loss: 2336.89; negative-loss: 12879.31 
(9563.81) Training batch 0/7151 loss: 0.05442366376519203
(9581.63) Training batch 500/7151 loss: 0.00010064167145173997
(9599.58) Training batch 1000/7151 loss: 0.001763921114616096
(9617.59) Training batch 1500/7151 loss: 0.00010860058682737872
(9635.63) Training batch 2000/7151 loss: 0.04153649881482124
(9653.73) Training batch 2500/7151 loss: 0.01309150829911232
(9671.84) Training batch 3000/7151 loss: 0.04893675446510315
(9689.96) Training batch 3500/7151 loss: 0.0006730247405357659
(9708.10) Training batch 4000/7151 loss: 0.00015830488700885326
(9726.24) Training batch 4500/7151 loss: 0.011338826268911362
(9744.39) Training batch 5000/7151 loss: 0.00013389854575507343
(9762.54) Training batch 5500/7151 loss: 0.0001443608052795753
(9780.69) Training batch 6000/7151 loss: 0.0006204525707289577
(9798.84) Training batch 6500/7151 loss: 0.03084016963839531
(9817.00) Training batch 7000/7151 loss: 0.0034625185653567314
(9822.43) Epoch 24 on training dataset: loss: 583.1921339550208; positive-loss: 532.16; negative-loss: 51.04 
(9822.43)------------- Training epoch 25 started -------------
(9937.67) Epoch 25 on test dataset: MRR: 0.52; Recalls: tensor([0.3642, 0.7069, 0.7851]) Loss: 15773.83positive-loss: 2306.55; negative-loss: 13467.28 
(9937.71) Training batch 0/7151 loss: 0.0023553415667265654
(9955.53) Training batch 500/7151 loss: 0.000108179978269618
(9973.49) Training batch 1000/7151 loss: 0.0001553295733174309
(9991.50) Training batch 1500/7151 loss: 0.00010461305646458641
(10009.55) Training batch 2000/7151 loss: 0.0001893619482871145
(10027.65) Training batch 2500/7151 loss: 0.0034071544650942087
(10045.76) Training batch 3000/7151 loss: 0.0001551549939904362
(10063.89) Training batch 3500/7151 loss: 0.00023235091066453606
(10082.02) Training batch 4000/7151 loss: 0.0021556629799306393
(10100.16) Training batch 4500/7151 loss: 0.00011887573782587424
(10118.31) Training batch 5000/7151 loss: 8.911619806895033e-05
(10136.46) Training batch 5500/7151 loss: 0.01821477711200714
(10154.61) Training batch 6000/7151 loss: 9.673989552538842e-05
(10172.76) Training batch 6500/7151 loss: 0.006899968720972538
(10190.91) Training batch 7000/7151 loss: 0.0001460793864680454
(10196.33) Epoch 25 on training dataset: loss: 573.1053332511146; positive-loss: 524.97; negative-loss: 48.14 
(10196.33)------------- Training epoch 26 started -------------
(10311.47) Epoch 26 on test dataset: MRR: 0.52; Recalls: tensor([0.3732, 0.6942, 0.7802]) Loss: 16230.91positive-loss: 1909.84; negative-loss: 14321.07 
(10311.51) Training batch 0/7151 loss: 0.00034029633388854563
(10329.32) Training batch 500/7151 loss: 9.613772272132337e-05
(10347.26) Training batch 1000/7151 loss: 0.00010434075375087559
(10365.27) Training batch 1500/7151 loss: 8.992882067104802e-05
(10383.33) Training batch 2000/7151 loss: 0.0002574738464318216
(10401.41) Training batch 2500/7151 loss: 0.10295365750789642
(10419.51) Training batch 3000/7151 loss: 0.0001410287368344143
(10437.62) Training batch 3500/7151 loss: 0.0010705499444156885
(10455.76) Training batch 4000/7151 loss: 0.00029012729646638036
(10473.89) Training batch 4500/7151 loss: 0.005343738477677107
(10492.02) Training batch 5000/7151 loss: 9.069030056707561e-05
(10510.17) Training batch 5500/7151 loss: 0.0001787440269254148
(10528.31) Training batch 6000/7151 loss: 8.696786244399846e-05
(10546.46) Training batch 6500/7151 loss: 0.012825009413063526
(10564.60) Training batch 7000/7151 loss: 0.0001068111159838736
(10570.02) Epoch 26 on training dataset: loss: 519.1529078558706; positive-loss: 475.28; negative-loss: 43.88 
(10570.02)------------- Training epoch 27 started -------------
(10685.00) Epoch 27 on test dataset: MRR: 0.52; Recalls: tensor([0.3720, 0.6983, 0.7887]) Loss: 13420.74positive-loss: 2550.54; negative-loss: 10870.19 
(10685.04) Training batch 0/7151 loss: 0.0363224558532238
(10702.86) Training batch 500/7151 loss: 8.742761565372348e-05
(10720.78) Training batch 1000/7151 loss: 0.00020638796559069306
(10738.78) Training batch 1500/7151 loss: 0.00023113969655241817
(10756.83) Training batch 2000/7151 loss: 0.0005909636383876204
(10774.91) Training batch 2500/7151 loss: 0.025878485292196274
(10793.00) Training batch 3000/7151 loss: 0.00019385352788958699
(10811.10) Training batch 3500/7151 loss: 0.0010377754224464297
(10829.22) Training batch 4000/7151 loss: 0.011817376129329205
(10847.33) Training batch 4500/7151 loss: 0.00014643064059782773
(10865.46) Training batch 5000/7151 loss: 8.211404929170385e-05
(10883.59) Training batch 5500/7151 loss: 0.0005419309600256383
(10901.72) Training batch 6000/7151 loss: 8.268495002994314e-05
(10919.85) Training batch 6500/7151 loss: 0.09644684940576553
(10937.98) Training batch 7000/7151 loss: 9.840549319051206e-05
(10943.41) Epoch 27 on training dataset: loss: 513.0941636866046; positive-loss: 467.15; negative-loss: 45.94 
(10943.41)------------- Training epoch 28 started -------------
(11058.58) Epoch 28 on test dataset: MRR: 0.52; Recalls: tensor([0.3840, 0.6998, 0.7821]) Loss: 18324.69positive-loss: 1744.92; negative-loss: 16579.77 
(11058.62) Training batch 0/7151 loss: 0.02049025520682335
(11076.44) Training batch 500/7151 loss: 8.293777500512078e-05
(11094.37) Training batch 1000/7151 loss: 0.0001919838978210464
(11112.37) Training batch 1500/7151 loss: 9.623374353395775e-05
(11130.43) Training batch 2000/7151 loss: 0.00019594324112404138
(11148.52) Training batch 2500/7151 loss: 0.010107893496751785
(11166.62) Training batch 3000/7151 loss: 0.00011025989078916609
(11184.73) Training batch 3500/7151 loss: 0.0002878002414945513
(11202.84) Training batch 4000/7151 loss: 0.002753906650468707
(11220.95) Training batch 4500/7151 loss: 0.0007853420102037489
(11239.06) Training batch 5000/7151 loss: 6.404610758181661e-05
(11257.17) Training batch 5500/7151 loss: 0.0002067535388050601
(11275.28) Training batch 6000/7151 loss: 8.020291716093197e-05
(11293.41) Training batch 6500/7151 loss: 0.0008432884351350367
(11311.54) Training batch 7000/7151 loss: 8.169016655301675e-05
(11316.96) Epoch 28 on training dataset: loss: 473.34585085882645; positive-loss: 432.33; negative-loss: 41.02 
(11316.96)------------- Training epoch 29 started -------------
(11432.19) Epoch 29 on test dataset: MRR: 0.52; Recalls: tensor([0.3679, 0.7035, 0.7880]) Loss: 17006.98positive-loss: 2183.58; negative-loss: 14823.40 
(11432.23) Training batch 0/7151 loss: 0.02051546424627304
(11450.05) Training batch 500/7151 loss: 8.142688602674752e-05
(11467.97) Training batch 1000/7151 loss: 0.00011819018982350826
(11485.97) Training batch 1500/7151 loss: 7.7767763286829e-05
(11504.02) Training batch 2000/7151 loss: 0.00011826971604023129
(11522.10) Training batch 2500/7151 loss: 0.022514741867780685
(11540.19) Training batch 3000/7151 loss: 5.8937213907483965e-05
(11558.28) Training batch 3500/7151 loss: 0.003932456485927105
(11576.38) Training batch 4000/7151 loss: 0.004843325819820166
(11594.50) Training batch 4500/7151 loss: 0.00024261207727249712
(11612.62) Training batch 5000/7151 loss: 7.15148780727759e-05
(11630.73) Training batch 5500/7151 loss: 0.036801181733608246
(11648.84) Training batch 6000/7151 loss: 6.185913662193343e-05
(11666.96) Training batch 6500/7151 loss: 0.0003498235309962183
(11685.07) Training batch 7000/7151 loss: 0.00015674612950533628
(11690.49) Epoch 29 on training dataset: loss: 453.3270111607526; positive-loss: 414.99; negative-loss: 38.34 
Model saved to cross_encoder.pt
(12019.99) Final test on test dataset!MRR: 0.50; Recalls: tensor([0.3520, 0.6851, 0.7728]) Loss: 19329.65positive-loss: 435.48; negative-loss: 18894.17 
