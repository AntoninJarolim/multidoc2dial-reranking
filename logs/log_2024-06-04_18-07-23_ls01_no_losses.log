Current time: 2024-06-04_18-07-23
train: True
num_epochs: 1
stop_time: None
load_model_path: None
save_model_path: ls_1.pt
bert_model_name: FacebookAI/xlm-roberta-base
lr: 1e-05
weight_decay: 0.1
dropout_rate: 0.1
label_smoothing: 0.1
gradient_clip: 0
compute_recall_at_k: False
(2.90)------------- Training epoch 0 started -------------
(244.36) Epoch 0 on test dataset: MRR: 0.11; Recalls: tensor([0.0365, 0.1289, 0.2480]) Loss: 6360.17positive-loss: 1922.91; negative-loss: 4437.26 
(244.68) Training batch 0/7151 loss: 1.6092979907989502
(385.68) Training batch 500/7151 loss: 1.2723252773284912
(526.91) Training batch 1000/7151 loss: 1.2634086608886719
(668.21) Training batch 1500/7151 loss: 1.3771767616271973
(809.12) Training batch 2000/7151 loss: 1.3869855403900146
(950.11) Training batch 2500/7151 loss: 1.4884556531906128
(1091.11) Training batch 3000/7151 loss: 1.4727450609207153
(1232.15) Training batch 3500/7151 loss: 1.4769481420516968
(1373.24) Training batch 4000/7151 loss: 1.5297240018844604
(1514.19) Training batch 4500/7151 loss: 1.4990142583847046
(1655.01) Training batch 5000/7151 loss: 1.4641222953796387
(1795.71) Training batch 5500/7151 loss: 1.5046852827072144
(1936.36) Training batch 6000/7151 loss: 1.488847255706787
(2076.94) Training batch 6500/7151 loss: 1.5093486309051514
(2217.51) Training batch 7000/7151 loss: 1.450713038444519
(2259.51) Epoch 0 on training dataset: loss: 0; positive-loss: 0.00; negative-loss: 0.00 
Model saved to ls_1.pt
(3650.86) Final test on test dataset!MRR: 0.03; Recalls: tensor([0.0085, 0.0215, 0.0456]) Loss: 8969.17positive-loss: 1402.43; negative-loss: 7566.74 
