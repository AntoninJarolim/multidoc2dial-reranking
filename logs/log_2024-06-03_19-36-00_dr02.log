train: True
num_epochs: 30
stop_time: 82800
load_model_path: None
save_model_path: ce_lr1e-5_wd1e-1_dr0.2.pt
bert_model_name: FacebookAI/xlm-roberta-base
lr: 1e-05
weight_decay: 0.1
dropout_rate: 0.2
compute_recall_at_k: False
(24.42)------------- Training epoch 0 started -------------
(260.40) Epoch 0 on test dataset: MRR: 0.10; Recalls: tensor([0.0156, 0.1113, 0.2721]) Loss: 5446.83positive-loss: 2451.94; negative-loss: 2994.89 
(261.60) Training batch 0/7151 loss: 1.8338627815246582
(390.19) Training batch 500/7151 loss: 0.7800061702728271
(518.80) Training batch 1000/7151 loss: 1.1157312393188477
(647.39) Training batch 1500/7151 loss: 0.3549898862838745
(775.98) Training batch 2000/7151 loss: 1.2392680644989014
(906.27) Training batch 2500/7151 loss: 1.1275713443756104
(1034.80) Training batch 3000/7151 loss: 0.5521650910377502
(1163.32) Training batch 3500/7151 loss: 0.5349498391151428
(1291.85) Training batch 4000/7151 loss: 1.0386680364608765
(1420.36) Training batch 4500/7151 loss: 0.5620616674423218
(1548.90) Training batch 5000/7151 loss: 0.5485389828681946
(1677.41) Training batch 5500/7151 loss: 0.6523656249046326
(1805.96) Training batch 6000/7151 loss: 0.44712650775909424
(1934.48) Training batch 6500/7151 loss: 0.7823137044906616
(2063.01) Training batch 7000/7151 loss: 0.6683233976364136
(2101.41) Epoch 0 on training dataset: loss: 36789.492334330454; positive-loss: 33393.66; negative-loss: 3395.83 
(2101.42)------------- Training epoch 1 started -------------
(2332.98) Epoch 1 on test dataset: MRR: 0.46; Recalls: tensor([0.3242, 0.6139, 0.7116]) Loss: 14969.31positive-loss: 449.82; negative-loss: 14519.50 
(2333.25) Training batch 0/7151 loss: 3.2466320991516113
(2461.74) Training batch 500/7151 loss: 0.36079323291778564
(2590.24) Training batch 1000/7151 loss: 1.0314321517944336
(2718.74) Training batch 1500/7151 loss: 0.2581590712070465
(2847.26) Training batch 2000/7151 loss: 0.42324531078338623
(2976.19) Training batch 2500/7151 loss: 0.9901467561721802
(3104.65) Training batch 3000/7151 loss: 0.4470905065536499
(3233.11) Training batch 3500/7151 loss: 0.21642659604549408
(3361.57) Training batch 4000/7151 loss: 0.9546815156936646
(3490.02) Training batch 4500/7151 loss: 0.8950976729393005
(3618.47) Training batch 5000/7151 loss: 0.515213131904602
(3746.93) Training batch 5500/7151 loss: 0.3908078670501709
(3875.38) Training batch 6000/7151 loss: 0.2675488591194153
(4003.82) Training batch 6500/7151 loss: 0.7867081165313721
(4132.27) Training batch 7000/7151 loss: 0.8304101228713989
(4170.66) Epoch 1 on training dataset: loss: 30485.576817715308; positive-loss: 27778.13; negative-loss: 2707.44 
(4170.66)------------- Training epoch 2 started -------------
(4402.04) Epoch 2 on test dataset: MRR: 0.46; Recalls: tensor([0.3424, 0.6048, 0.6940]) Loss: 19456.30positive-loss: 271.16; negative-loss: 19185.14 
(4402.31) Training batch 0/7151 loss: 2.9263787269592285
(4530.80) Training batch 500/7151 loss: 0.20933912694454193
(4659.32) Training batch 1000/7151 loss: 1.2748597860336304
(4787.83) Training batch 1500/7151 loss: 0.1739814728498459
(4916.32) Training batch 2000/7151 loss: 0.45902204513549805
(5044.81) Training batch 2500/7151 loss: 0.8435211777687073
(5173.28) Training batch 3000/7151 loss: 0.36009681224823
(5301.77) Training batch 3500/7151 loss: 0.21959523856639862
(5430.25) Training batch 4000/7151 loss: 0.9417966604232788
(5558.74) Training batch 4500/7151 loss: 0.3587150573730469
(5687.24) Training batch 5000/7151 loss: 0.19387023150920868
(5815.73) Training batch 5500/7151 loss: 0.3266350030899048
(5944.20) Training batch 6000/7151 loss: 0.1812283992767334
(6072.69) Training batch 6500/7151 loss: 0.7798017263412476
(6201.18) Training batch 7000/7151 loss: 0.8007402420043945
(6239.57) Epoch 2 on training dataset: loss: 26391.77956262871; positive-loss: 24013.68; negative-loss: 2378.10 
(6239.57)------------- Training epoch 3 started -------------
(6471.06) Epoch 3 on test dataset: MRR: 0.46; Recalls: tensor([0.3438, 0.5885, 0.6999]) Loss: 19007.43positive-loss: 251.16; negative-loss: 18756.27 
(6471.33) Training batch 0/7151 loss: 1.280049204826355
(6599.79) Training batch 500/7151 loss: 0.13749819993972778
(6728.26) Training batch 1000/7151 loss: 1.0258058309555054
(6856.72) Training batch 1500/7151 loss: 0.13285638391971588
(6985.16) Training batch 2000/7151 loss: 0.14620612561702728
(7113.59) Training batch 2500/7151 loss: 0.6796988248825073
(7242.05) Training batch 3000/7151 loss: 0.41410088539123535
(7370.54) Training batch 3500/7151 loss: 0.10068619251251221
(7499.04) Training batch 4000/7151 loss: 0.7847735285758972
(7627.53) Training batch 4500/7151 loss: 0.3677029013633728
(7756.00) Training batch 5000/7151 loss: 0.15315571427345276
(7884.46) Training batch 5500/7151 loss: 0.16241568326950073
(8012.95) Training batch 6000/7151 loss: 0.16757068037986755
(8141.43) Training batch 6500/7151 loss: 0.6661137342453003
(8270.15) Training batch 7000/7151 loss: 0.6849891543388367
(8310.29) Epoch 3 on training dataset: loss: 22265.076129616587; positive-loss: 20188.10; negative-loss: 2076.97 
(8310.29)------------- Training epoch 4 started -------------
(8546.60) Epoch 4 on test dataset: MRR: 0.45; Recalls: tensor([0.3301, 0.5788, 0.6901]) Loss: 29858.91positive-loss: 225.77; negative-loss: 29633.14 
(8546.86) Training batch 0/7151 loss: 0.879094123840332
(8675.75) Training batch 500/7151 loss: 0.08055649697780609
(8804.27) Training batch 1000/7151 loss: 1.0690311193466187
(8932.81) Training batch 1500/7151 loss: 0.07690659910440445
(9061.32) Training batch 2000/7151 loss: 0.07434060424566269
(9189.85) Training batch 2500/7151 loss: 0.7102527022361755
(9318.37) Training batch 3000/7151 loss: 0.4398882985115051
(9446.89) Training batch 3500/7151 loss: 0.16521696746349335
(9575.40) Training batch 4000/7151 loss: 0.6889787912368774
(9703.92) Training batch 4500/7151 loss: 0.13138918578624725
(9832.44) Training batch 5000/7151 loss: 0.09654653817415237
(9960.95) Training batch 5500/7151 loss: 0.130089670419693
(10089.47) Training batch 6000/7151 loss: 0.09984768182039261
(10218.02) Training batch 6500/7151 loss: 0.5055552124977112
(10346.56) Training batch 7000/7151 loss: 0.5290729999542236
(10384.97) Epoch 4 on training dataset: loss: 18893.536917299032; positive-loss: 17047.73; negative-loss: 1845.81 
(10384.97)------------- Training epoch 5 started -------------
(10616.82) Epoch 5 on test dataset: MRR: 0.41; Recalls: tensor([0.2826, 0.5456, 0.6615]) Loss: 45491.47positive-loss: 105.23; negative-loss: 45386.24 
(10617.08) Training batch 0/7151 loss: 1.4677540063858032
(10745.57) Training batch 500/7151 loss: 0.05709224194288254
(10874.05) Training batch 1000/7151 loss: 0.5020520687103271
(11002.53) Training batch 1500/7151 loss: 0.0676274448633194
(11131.01) Training batch 2000/7151 loss: 0.12437985837459564
(11259.47) Training batch 2500/7151 loss: 0.5834339261054993
(11387.94) Training batch 3000/7151 loss: 0.4869387149810791
(11516.41) Training batch 3500/7151 loss: 0.04523167386651039
(11644.91) Training batch 4000/7151 loss: 0.5038207173347473
(11773.42) Training batch 4500/7151 loss: 0.03790214657783508
(11901.92) Training batch 5000/7151 loss: 0.0403900146484375
(12030.38) Training batch 5500/7151 loss: 0.17231948673725128
(12158.87) Training batch 6000/7151 loss: 0.031392961740493774
(12287.35) Training batch 6500/7151 loss: 0.42859962582588196
(12415.82) Training batch 7000/7151 loss: 0.4829082787036896
(12454.21) Epoch 5 on training dataset: loss: 16345.232967682223; positive-loss: 14678.93; negative-loss: 1666.30 
(12454.21)------------- Training epoch 6 started -------------
(12686.15) Epoch 6 on test dataset: MRR: 0.44; Recalls: tensor([0.3151, 0.5658, 0.6849]) Loss: 48876.22positive-loss: 162.08; negative-loss: 48714.14 
(12686.41) Training batch 0/7151 loss: 0.830956220626831
(12814.85) Training batch 500/7151 loss: 0.040779031813144684
(12943.31) Training batch 1000/7151 loss: 0.7882217764854431
(13071.79) Training batch 1500/7151 loss: 0.11871888488531113
(13200.28) Training batch 2000/7151 loss: 0.08310111612081528
(13328.76) Training batch 2500/7151 loss: 0.5544987320899963
(13457.22) Training batch 3000/7151 loss: 0.4599830210208893
(13585.70) Training batch 3500/7151 loss: 0.06818393617868423
(13714.20) Training batch 4000/7151 loss: 0.623408854007721
(13842.71) Training batch 4500/7151 loss: 0.10221323370933533
(13971.19) Training batch 5000/7151 loss: 0.06451752781867981
(14099.68) Training batch 5500/7151 loss: 0.17653517425060272
(14228.14) Training batch 6000/7151 loss: 0.09995521605014801
(14356.60) Training batch 6500/7151 loss: 0.5676968693733215
(14485.13) Training batch 7000/7151 loss: 0.3438684940338135
(14523.53) Epoch 6 on training dataset: loss: 14264.354616178189; positive-loss: 12750.84; negative-loss: 1513.52 
(14523.53)------------- Training epoch 7 started -------------
(14755.05) Epoch 7 on test dataset: MRR: 0.43; Recalls: tensor([0.3105, 0.5599, 0.6777]) Loss: 55188.60positive-loss: 142.88; negative-loss: 55045.72 
(14755.31) Training batch 0/7151 loss: 0.5226555466651917
(14883.84) Training batch 500/7151 loss: 0.024390816688537598
(15012.38) Training batch 1000/7151 loss: 0.15696221590042114
(15140.92) Training batch 1500/7151 loss: 0.042022548615932465
(15269.46) Training batch 2000/7151 loss: 0.07189777493476868
(15398.01) Training batch 2500/7151 loss: 0.8935380578041077
(15526.56) Training batch 3000/7151 loss: 0.6224743723869324
(15655.13) Training batch 3500/7151 loss: 0.05908839404582977
(15783.69) Training batch 4000/7151 loss: 0.6680065989494324
(15912.24) Training batch 4500/7151 loss: 0.04651994630694389
(16040.78) Training batch 5000/7151 loss: 0.040566835552453995
(16169.34) Training batch 5500/7151 loss: 0.0263606458902359
(16297.89) Training batch 6000/7151 loss: 0.027871962636709213
(16426.44) Training batch 6500/7151 loss: 0.39827799797058105
(16555.04) Training batch 7000/7151 loss: 0.3469301462173462
(16593.46) Epoch 7 on training dataset: loss: 12674.27874173385; positive-loss: 11277.53; negative-loss: 1396.75 
(16593.46)------------- Training epoch 8 started -------------
(16824.96) Epoch 8 on test dataset: MRR: 0.43; Recalls: tensor([0.3099, 0.5658, 0.6738]) Loss: 42540.50positive-loss: 179.92; negative-loss: 42360.58 
(16825.22) Training batch 0/7151 loss: 0.5225181579589844
(16953.68) Training batch 500/7151 loss: 0.022897519171237946
(17082.15) Training batch 1000/7151 loss: 0.29496660828590393
(17210.61) Training batch 1500/7151 loss: 0.008235037326812744
(17339.07) Training batch 2000/7151 loss: 0.08340819180011749
(17467.56) Training batch 2500/7151 loss: 0.5947154760360718
(17596.04) Training batch 3000/7151 loss: 0.3515055179595947
(17724.54) Training batch 3500/7151 loss: 0.011929572559893131
(17853.01) Training batch 4000/7151 loss: 0.3564779460430145
(17981.50) Training batch 4500/7151 loss: 0.20519423484802246
(18110.02) Training batch 5000/7151 loss: 0.15110091865062714
(18238.49) Training batch 5500/7151 loss: 0.13497471809387207
(18366.96) Training batch 6000/7151 loss: 0.3039553463459015
(18495.42) Training batch 6500/7151 loss: 0.47188326716423035
(18623.87) Training batch 7000/7151 loss: 0.39322349429130554
(18662.25) Epoch 8 on training dataset: loss: 11525.093355697429; positive-loss: 10227.36; negative-loss: 1297.73 
(18662.25)------------- Training epoch 9 started -------------
(18893.64) Epoch 9 on test dataset: MRR: 0.43; Recalls: tensor([0.3047, 0.5553, 0.6803]) Loss: 59681.45positive-loss: 113.87; negative-loss: 59567.58 
(18893.90) Training batch 0/7151 loss: 0.3875211179256439
(19022.34) Training batch 500/7151 loss: 0.0044873799197375774
(19150.79) Training batch 1000/7151 loss: 0.14746855199337006
(19279.26) Training batch 1500/7151 loss: 0.06344413757324219
(19407.71) Training batch 2000/7151 loss: 0.09208894520998001
(19536.17) Training batch 2500/7151 loss: 1.3367141485214233
(19664.63) Training batch 3000/7151 loss: 0.32903748750686646
(19793.07) Training batch 3500/7151 loss: 0.0684187263250351
(19921.53) Training batch 4000/7151 loss: 0.3077136278152466
(20049.97) Training batch 4500/7151 loss: 0.015502950176596642
(20178.42) Training batch 5000/7151 loss: 0.04784077778458595
(20306.87) Training batch 5500/7151 loss: 0.601678729057312
(20435.34) Training batch 6000/7151 loss: 0.11926787346601486
(20563.80) Training batch 6500/7151 loss: 0.2875661551952362
(20692.29) Training batch 7000/7151 loss: 0.2108967900276184
(20730.68) Epoch 9 on training dataset: loss: 10721.087839251075; positive-loss: 9507.92; negative-loss: 1213.17 
(20730.68)------------- Training epoch 10 started -------------
(20962.24) Epoch 10 on test dataset: MRR: 0.43; Recalls: tensor([0.3021, 0.5579, 0.6771]) Loss: 50152.90positive-loss: 143.14; negative-loss: 50009.77 
(20962.50) Training batch 0/7151 loss: 0.22027920186519623
(21090.94) Training batch 500/7151 loss: 0.002177941845729947
(21219.40) Training batch 1000/7151 loss: 0.2384055107831955
(21347.86) Training batch 1500/7151 loss: 0.1341569423675537
(21476.31) Training batch 2000/7151 loss: 0.07410641759634018
(21604.76) Training batch 2500/7151 loss: 0.43949925899505615
(21733.21) Training batch 3000/7151 loss: 0.30630195140838623
(21861.67) Training batch 3500/7151 loss: 0.00289342412725091
(21990.11) Training batch 4000/7151 loss: 0.37310120463371277
(22118.56) Training batch 4500/7151 loss: 0.020793231204152107
(22247.05) Training batch 5000/7151 loss: 0.0074598356150090694
(22375.50) Training batch 5500/7151 loss: 0.02558017708361149
(22503.94) Training batch 6000/7151 loss: 0.1106514260172844
(22632.38) Training batch 6500/7151 loss: 0.16985711455345154
(22760.82) Training batch 7000/7151 loss: 0.19861368834972382
(22799.19) Epoch 10 on training dataset: loss: 9857.43251632926; positive-loss: 8729.32; negative-loss: 1128.12 
(22799.19)------------- Training epoch 11 started -------------
(23030.87) Epoch 11 on test dataset: MRR: 0.45; Recalls: tensor([0.3236, 0.5990, 0.7142]) Loss: 57055.65positive-loss: 125.20; negative-loss: 56930.46 
(23031.13) Training batch 0/7151 loss: 0.10558001697063446
(23159.56) Training batch 500/7151 loss: 0.005986220669001341
(23288.02) Training batch 1000/7151 loss: 0.3381194472312927
(23416.49) Training batch 1500/7151 loss: 0.005914101377129555
(23545.01) Training batch 2000/7151 loss: 0.05006260797381401
(23673.51) Training batch 2500/7151 loss: 0.5806813836097717
(23802.06) Training batch 3000/7151 loss: 0.3091316819190979
(23930.61) Training batch 3500/7151 loss: 0.028750624507665634
(24059.16) Training batch 4000/7151 loss: 0.2805163860321045
(24187.70) Training batch 4500/7151 loss: 0.16079813241958618
(24316.24) Training batch 5000/7151 loss: 0.018183665350079536
(24444.78) Training batch 5500/7151 loss: 0.0068079037591814995
(24573.35) Training batch 6000/7151 loss: 0.013811730779707432
(24701.89) Training batch 6500/7151 loss: 0.5102775692939758
(24830.43) Training batch 7000/7151 loss: 0.3850807249546051
(24868.83) Epoch 11 on training dataset: loss: 8935.05472072122; positive-loss: 7878.99; negative-loss: 1056.06 
(24868.83)------------- Training epoch 12 started -------------
(25099.74) Epoch 12 on test dataset: MRR: 0.44; Recalls: tensor([0.3118, 0.5807, 0.6973]) Loss: 58917.31positive-loss: 125.94; negative-loss: 58791.37 
(25100.01) Training batch 0/7151 loss: 0.17520000040531158
(25228.47) Training batch 500/7151 loss: 0.005068197846412659
(25356.94) Training batch 1000/7151 loss: 0.1775631159543991
(25485.43) Training batch 1500/7151 loss: 0.016055148094892502
(25613.91) Training batch 2000/7151 loss: 0.05273371562361717
(25742.39) Training batch 2500/7151 loss: 0.6460234522819519
(25870.86) Training batch 3000/7151 loss: 0.40244045853614807
(25999.34) Training batch 3500/7151 loss: 0.005218649283051491
(26127.81) Training batch 4000/7151 loss: 0.1365831047296524
(26256.27) Training batch 4500/7151 loss: 0.06328906118869781
(26384.74) Training batch 5000/7151 loss: 0.023737629875540733
(26513.20) Training batch 5500/7151 loss: 0.11277865618467331
(26641.67) Training batch 6000/7151 loss: 0.11443483084440231
(26770.15) Training batch 6500/7151 loss: 1.1896253824234009
(26898.65) Training batch 7000/7151 loss: 0.11986974626779556
(26937.04) Epoch 12 on training dataset: loss: 8429.961222023343; positive-loss: 7444.30; negative-loss: 985.66 
(26937.04)------------- Training epoch 13 started -------------
(27168.36) Epoch 13 on test dataset: MRR: 0.46; Recalls: tensor([0.3333, 0.5970, 0.6940]) Loss: 45095.09positive-loss: 221.01; negative-loss: 44874.08 
(27168.62) Training batch 0/7151 loss: 0.2120884507894516
(27297.11) Training batch 500/7151 loss: 0.00016264664009213448
(27425.62) Training batch 1000/7151 loss: 2.165609836578369
(27554.12) Training batch 1500/7151 loss: 0.031535230576992035
(27682.64) Training batch 2000/7151 loss: 0.022908950224518776
(27811.16) Training batch 2500/7151 loss: 0.7659929394721985
(27939.63) Training batch 3000/7151 loss: 0.2819376289844513
(28068.10) Training batch 3500/7151 loss: 0.0048656403087079525
(28196.58) Training batch 4000/7151 loss: 0.3277495503425598
(28325.09) Training batch 4500/7151 loss: 0.020455429330468178
(28453.58) Training batch 5000/7151 loss: 0.03384732827544212
(28582.10) Training batch 5500/7151 loss: 0.009802241809666157
(28715.74) Training batch 6000/7151 loss: 0.03733031824231148
(28844.32) Training batch 6500/7151 loss: 0.24766899645328522
(28972.81) Training batch 7000/7151 loss: 0.20324791967868805
(29011.20) Epoch 13 on training dataset: loss: 7835.316686969054; positive-loss: 6901.06; negative-loss: 934.26 
(29011.20)------------- Training epoch 14 started -------------
(29242.80) Epoch 14 on test dataset: MRR: 0.44; Recalls: tensor([0.3184, 0.5697, 0.6556]) Loss: 59210.48positive-loss: 147.26; negative-loss: 59063.22 
(29243.06) Training batch 0/7151 loss: 0.13999146223068237
(29371.51) Training batch 500/7151 loss: 0.0016644411953166127
(29499.95) Training batch 1000/7151 loss: 0.14330294728279114
(29628.42) Training batch 1500/7151 loss: 0.009556696750223637
(29756.90) Training batch 2000/7151 loss: 0.1399669349193573
(29885.38) Training batch 2500/7151 loss: 0.6904666423797607
(30013.87) Training batch 3000/7151 loss: 0.8813164830207825
(30142.36) Training batch 3500/7151 loss: 0.024556556716561317
(30270.86) Training batch 4000/7151 loss: 0.7252798676490784
(30399.34) Training batch 4500/7151 loss: 0.04019462689757347
(30527.83) Training batch 5000/7151 loss: 0.023620879277586937
(30656.32) Training batch 5500/7151 loss: 0.005667062010616064
(30784.81) Training batch 6000/7151 loss: 0.09050330519676208
(30913.33) Training batch 6500/7151 loss: 0.07632101327180862
(31041.82) Training batch 7000/7151 loss: 0.13793480396270752
(31080.22) Epoch 14 on training dataset: loss: 7220.961595497914; positive-loss: 6352.29; negative-loss: 868.67 
(31080.22)------------- Training epoch 15 started -------------
(31311.74) Epoch 15 on test dataset: MRR: 0.45; Recalls: tensor([0.3223, 0.5951, 0.6999]) Loss: 45810.48positive-loss: 219.95; negative-loss: 45590.53 
(31312.00) Training batch 0/7151 loss: 0.5021661520004272
(31440.46) Training batch 500/7151 loss: 0.021276721730828285
(31568.92) Training batch 1000/7151 loss: 0.26108407974243164
(31697.41) Training batch 1500/7151 loss: 0.08234554529190063
(31825.91) Training batch 2000/7151 loss: 0.14462809264659882
(31954.41) Training batch 2500/7151 loss: 0.2903141379356384
(32082.93) Training batch 3000/7151 loss: 0.29038622975349426
(32211.44) Training batch 3500/7151 loss: 0.017509955912828445
(32339.95) Training batch 4000/7151 loss: 0.0831889733672142
(32468.47) Training batch 4500/7151 loss: 0.0338483527302742
(32596.97) Training batch 5000/7151 loss: 0.026124686002731323
(32725.42) Training batch 5500/7151 loss: 0.014569160528481007
(32853.88) Training batch 6000/7151 loss: 0.0781741514801979
(32982.34) Training batch 6500/7151 loss: 0.09943375736474991
(33110.79) Training batch 7000/7151 loss: 0.20356054604053497
(33149.17) Epoch 15 on training dataset: loss: 6957.372509638745; positive-loss: 6122.59; negative-loss: 834.78 
(33149.17)------------- Training epoch 16 started -------------
(33380.66) Epoch 16 on test dataset: MRR: 0.44; Recalls: tensor([0.3184, 0.5807, 0.6849]) Loss: 45609.96positive-loss: 208.40; negative-loss: 45401.56 
(33380.92) Training batch 0/7151 loss: 0.37896957993507385
(33509.36) Training batch 500/7151 loss: 0.003015784313902259
(33637.80) Training batch 1000/7151 loss: 0.14162348210811615
(33766.24) Training batch 1500/7151 loss: 0.0010371282696723938
(33894.70) Training batch 2000/7151 loss: 0.03421928361058235
(34023.14) Training batch 2500/7151 loss: 0.5145756602287292
(34151.62) Training batch 3000/7151 loss: 0.3073351979255676
(34280.05) Training batch 3500/7151 loss: 0.00865273829549551
(34408.49) Training batch 4000/7151 loss: 0.3055991530418396
(34536.96) Training batch 4500/7151 loss: 0.014747995883226395
(34665.44) Training batch 5000/7151 loss: 0.023366086184978485
(34793.90) Training batch 5500/7151 loss: 0.08338683098554611
(34922.37) Training batch 6000/7151 loss: 0.0013111894950270653
(35050.83) Training batch 6500/7151 loss: 0.15453043580055237
(35179.29) Training batch 7000/7151 loss: 0.23952969908714294
(35217.67) Epoch 16 on training dataset: loss: 6607.272650556338; positive-loss: 5813.09; negative-loss: 794.19 
(35217.68)------------- Training epoch 17 started -------------
(35449.40) Epoch 17 on test dataset: MRR: 0.45; Recalls: tensor([0.3216, 0.5996, 0.7025]) Loss: 42657.07positive-loss: 259.85; negative-loss: 42397.21 
(35449.67) Training batch 0/7151 loss: 0.2197972983121872
(35578.14) Training batch 500/7151 loss: 0.005804979708045721
(35706.62) Training batch 1000/7151 loss: 0.07738055288791656
(35835.10) Training batch 1500/7151 loss: 0.06636760383844376
(35963.58) Training batch 2000/7151 loss: 0.08379724621772766
(36092.09) Training batch 2500/7151 loss: 1.3442548513412476
(36220.58) Training batch 3000/7151 loss: 0.15036554634571075
(36349.05) Training batch 3500/7151 loss: 0.04089730605483055
(36477.53) Training batch 4000/7151 loss: 0.23632235825061798
(36606.00) Training batch 4500/7151 loss: 0.012631760910153389
(36734.45) Training batch 5000/7151 loss: 0.00600226828828454
(36862.93) Training batch 5500/7151 loss: 0.0255732499063015
(36991.41) Training batch 6000/7151 loss: 0.007983236573636532
(37119.85) Training batch 6500/7151 loss: 0.0855916291475296
(37248.29) Training batch 7000/7151 loss: 0.12495983392000198
(37286.68) Epoch 17 on training dataset: loss: 6237.262131867956; positive-loss: 5492.46; negative-loss: 744.80 
(37286.68)------------- Training epoch 18 started -------------
(37517.46) Epoch 18 on test dataset: MRR: 0.45; Recalls: tensor([0.3197, 0.6003, 0.7038]) Loss: 46840.32positive-loss: 273.24; negative-loss: 46567.08 
(37517.72) Training batch 0/7151 loss: 0.13991905748844147
(37646.16) Training batch 500/7151 loss: 0.004069736693054438
(37774.61) Training batch 1000/7151 loss: 0.10150637477636337
(37903.08) Training batch 1500/7151 loss: 0.07061661779880524
(38031.56) Training batch 2000/7151 loss: 0.014022951014339924
(38160.04) Training batch 2500/7151 loss: 1.4093306064605713
(38288.54) Training batch 3000/7151 loss: 0.1400437355041504
(38417.03) Training batch 3500/7151 loss: 0.008589835837483406
(38545.53) Training batch 4000/7151 loss: 0.054762836545705795
(38674.01) Training batch 4500/7151 loss: 0.05991404876112938
(38802.46) Training batch 5000/7151 loss: 0.009377449750900269
(38930.94) Training batch 5500/7151 loss: 0.040569353848695755
(39059.41) Training batch 6000/7151 loss: 0.0011208833893761039
(39187.89) Training batch 6500/7151 loss: 0.21630249917507172
(39316.35) Training batch 7000/7151 loss: 0.13585269451141357
(39354.74) Epoch 18 on training dataset: loss: 6063.926711153172; positive-loss: 5342.17; negative-loss: 721.76 
(39354.74)------------- Training epoch 19 started -------------
(39586.33) Epoch 19 on test dataset: MRR: 0.46; Recalls: tensor([0.3327, 0.5924, 0.6888]) Loss: 54945.11positive-loss: 209.91; negative-loss: 54735.20 
(39586.59) Training batch 0/7151 loss: 0.25333476066589355
(39715.06) Training batch 500/7151 loss: 0.076839379966259
(39843.56) Training batch 1000/7151 loss: 0.19235455989837646
(39972.05) Training batch 1500/7151 loss: 0.004850161727517843
(40100.51) Training batch 2000/7151 loss: 0.04952894151210785
(40228.99) Training batch 2500/7151 loss: 0.8900903463363647
(40357.48) Training batch 3000/7151 loss: 0.06712900102138519
(40485.98) Training batch 3500/7151 loss: 0.03817528858780861
(40614.46) Training batch 4000/7151 loss: 0.055151816457509995
(40742.96) Training batch 4500/7151 loss: 0.0010564280673861504
(40871.47) Training batch 5000/7151 loss: 0.008612759411334991
(40999.99) Training batch 5500/7151 loss: 0.007619032636284828
(41128.47) Training batch 6000/7151 loss: 0.0003809499612543732
(41256.97) Training batch 6500/7151 loss: 0.4450564682483673
(41385.48) Training batch 7000/7151 loss: 1.1504181623458862
(41423.88) Epoch 19 on training dataset: loss: 5483.811484514058; positive-loss: 4808.80; negative-loss: 675.01 
(41423.88)------------- Training epoch 20 started -------------
(41654.81) Epoch 20 on test dataset: MRR: 0.44; Recalls: tensor([0.3242, 0.5671, 0.6647]) Loss: 66970.74positive-loss: 145.44; negative-loss: 66825.30 
(41655.08) Training batch 0/7151 loss: 0.4196922481060028
(41783.53) Training batch 500/7151 loss: 0.00028972604195587337
(41912.02) Training batch 1000/7151 loss: 0.3303578495979309
(42040.52) Training batch 1500/7151 loss: 0.023491108790040016
(42169.01) Training batch 2000/7151 loss: 0.09900073707103729
(42297.47) Training batch 2500/7151 loss: 0.3717072308063507
(42425.94) Training batch 3000/7151 loss: 0.03122781403362751
(42554.42) Training batch 3500/7151 loss: 0.07580668479204178
(42682.88) Training batch 4000/7151 loss: 0.16586142778396606
(42811.35) Training batch 4500/7151 loss: 0.0008407936547882855
(42939.79) Training batch 5000/7151 loss: 0.01284200232475996
(43068.25) Training batch 5500/7151 loss: 0.10975120216608047
(43196.72) Training batch 6000/7151 loss: 0.0007693261140957475
(43325.21) Training batch 6500/7151 loss: 0.16878724098205566
(43453.68) Training batch 7000/7151 loss: 0.30008819699287415
(43492.06) Epoch 20 on training dataset: loss: 5320.0512916184225; positive-loss: 4676.30; negative-loss: 643.75 
(43492.06)------------- Training epoch 21 started -------------
(43722.88) Epoch 21 on test dataset: MRR: 0.44; Recalls: tensor([0.3216, 0.5625, 0.6458]) Loss: 73119.06positive-loss: 126.97; negative-loss: 72992.09 
(43723.15) Training batch 0/7151 loss: 0.06438229233026505
(43851.62) Training batch 500/7151 loss: 0.008190518245100975
(43980.11) Training batch 1000/7151 loss: 0.05653775483369827
(44108.62) Training batch 1500/7151 loss: 0.044873807579278946
(44237.12) Training batch 2000/7151 loss: 0.0703677162528038
(44365.61) Training batch 2500/7151 loss: 0.09544570744037628
(44494.13) Training batch 3000/7151 loss: 0.04749467968940735
(44622.66) Training batch 3500/7151 loss: 0.02257835865020752
(44751.17) Training batch 4000/7151 loss: 0.06207284331321716
(44879.67) Training batch 4500/7151 loss: 0.0633186399936676
(45008.14) Training batch 5000/7151 loss: 0.009992842562496662
(45136.61) Training batch 5500/7151 loss: 0.021483061835169792
(45265.06) Training batch 6000/7151 loss: 0.04206843674182892
(45393.51) Training batch 6500/7151 loss: 0.11877518147230148
(45521.96) Training batch 7000/7151 loss: 0.14072728157043457
(45560.34) Epoch 21 on training dataset: loss: 5221.592693133494; positive-loss: 4600.05; negative-loss: 621.55 
(45560.34)------------- Training epoch 22 started -------------
(45791.31) Epoch 22 on test dataset: MRR: 0.45; Recalls: tensor([0.3184, 0.5970, 0.7012]) Loss: 41696.77positive-loss: 209.46; negative-loss: 41487.30 
(45791.58) Training batch 0/7151 loss: 0.09202322363853455
(45920.02) Training batch 500/7151 loss: 0.0024491462390869856
(46048.46) Training batch 1000/7151 loss: 0.3025924265384674
(46176.90) Training batch 1500/7151 loss: 0.044134415686130524
(46305.35) Training batch 2000/7151 loss: 0.11132609844207764
(46433.78) Training batch 2500/7151 loss: 0.1190684363245964
(46562.23) Training batch 3000/7151 loss: 0.08045854419469833
(46690.69) Training batch 3500/7151 loss: 0.00679504219442606
(46819.14) Training batch 4000/7151 loss: 0.05314173921942711
(46947.63) Training batch 4500/7151 loss: 1.5728754997253418
(47076.13) Training batch 5000/7151 loss: 0.005734119098633528
(47204.62) Training batch 5500/7151 loss: 0.09903739392757416
(47333.09) Training batch 6000/7151 loss: 0.0009299461962655187
(47461.58) Training batch 6500/7151 loss: 0.12516368925571442
(47590.07) Training batch 7000/7151 loss: 0.2743409276008606
(47628.46) Epoch 22 on training dataset: loss: 4999.588533555838; positive-loss: 4392.95; negative-loss: 606.63 
(47628.46)------------- Training epoch 23 started -------------
(47859.28) Epoch 23 on test dataset: MRR: 0.42; Recalls: tensor([0.3151, 0.5143, 0.5697]) Loss: 73358.09positive-loss: 86.11; negative-loss: 73271.98 
(47859.54) Training batch 0/7151 loss: 0.09071794897317886
(47987.99) Training batch 500/7151 loss: 0.0002442128024995327
(48116.46) Training batch 1000/7151 loss: 0.07070710510015488
(48244.94) Training batch 1500/7151 loss: 0.14286595582962036
(48373.44) Training batch 2000/7151 loss: 0.02938825637102127
(48501.93) Training batch 2500/7151 loss: 0.18849630653858185
(48630.43) Training batch 3000/7151 loss: 0.01983111910521984
(48759.39) Training batch 3500/7151 loss: 0.20837125182151794
(48893.64) Training batch 4000/7151 loss: 0.10614916682243347
(49022.34) Training batch 4500/7151 loss: 0.009973734617233276
(49151.04) Training batch 5000/7151 loss: 0.8696030378341675
(49279.75) Training batch 5500/7151 loss: 0.10398496687412262
(49408.59) Training batch 6000/7151 loss: 0.0033773458562791348
(49537.31) Training batch 6500/7151 loss: 0.0588102750480175
(49666.28) Training batch 7000/7151 loss: 0.14787554740905762
(49704.84) Epoch 23 on training dataset: loss: 4802.554637708761; positive-loss: 4234.27; negative-loss: 568.29 
(49704.84)------------- Training epoch 24 started -------------
(49936.31) Epoch 24 on test dataset: MRR: 0.45; Recalls: tensor([0.3288, 0.5775, 0.6582]) Loss: 57995.92positive-loss: 157.00; negative-loss: 57838.92 
(49936.58) Training batch 0/7151 loss: 0.16834621131420135
(50065.36) Training batch 500/7151 loss: 0.0006158759933896363
(50194.24) Training batch 1000/7151 loss: 0.09787774085998535
(50323.34) Training batch 1500/7151 loss: 0.011187554337084293
(50452.23) Training batch 2000/7151 loss: 0.048866815865039825
(50581.46) Training batch 2500/7151 loss: 0.3552045524120331
(50710.60) Training batch 3000/7151 loss: 0.03167865052819252
(50839.72) Training batch 3500/7151 loss: 0.04560338333249092
(50969.02) Training batch 4000/7151 loss: 0.1226433739066124
(51098.44) Training batch 4500/7151 loss: 0.0024790666066110134
(51227.96) Training batch 5000/7151 loss: 0.010272395797073841
(51357.20) Training batch 5500/7151 loss: 0.015202435664832592
(51486.46) Training batch 6000/7151 loss: 0.09247037023305893
(51615.78) Training batch 6500/7151 loss: 0.19265516102313995
(51745.40) Training batch 7000/7151 loss: 0.1323675960302353
(51784.20) Epoch 24 on training dataset: loss: 4608.248117786776; positive-loss: 4059.15; negative-loss: 549.09 
(51784.20)------------- Training epoch 25 started -------------
(52015.56) Epoch 25 on test dataset: MRR: 0.45; Recalls: tensor([0.3275, 0.5866, 0.6745]) Loss: 52514.90positive-loss: 169.18; negative-loss: 52345.72 
(52015.83) Training batch 0/7151 loss: 0.14577500522136688
(52144.36) Training batch 500/7151 loss: 0.04569729045033455
(52273.07) Training batch 1000/7151 loss: 0.1901494562625885
(52401.61) Training batch 1500/7151 loss: 0.07620934396982193
(52530.44) Training batch 2000/7151 loss: 0.015448546968400478
(52659.30) Training batch 2500/7151 loss: 0.2266063690185547
(52788.47) Training batch 3000/7151 loss: 0.006094166077673435
(52917.15) Training batch 3500/7151 loss: 0.007200399413704872
(53046.26) Training batch 4000/7151 loss: 0.024676084518432617
(53176.98) Training batch 4500/7151 loss: 0.012295587919652462
(53305.65) Training batch 5000/7151 loss: 0.0062731243669986725
(53434.55) Training batch 5500/7151 loss: 0.026977410539984703
(53563.17) Training batch 6000/7151 loss: 0.002546490402892232
(53692.07) Training batch 6500/7151 loss: 0.177281454205513
(53820.72) Training batch 7000/7151 loss: 0.018038146197795868
(53859.36) Epoch 25 on training dataset: loss: 4607.248816022103; positive-loss: 4066.30; negative-loss: 540.95 
(53859.36)------------- Training epoch 26 started -------------
(54091.00) Epoch 26 on test dataset: MRR: 0.45; Recalls: tensor([0.3242, 0.5749, 0.6686]) Loss: 65538.36positive-loss: 129.03; negative-loss: 65409.33 
(54091.26) Training batch 0/7151 loss: 0.023361336439847946
(54219.91) Training batch 500/7151 loss: 0.009447977878153324
(54348.98) Training batch 1000/7151 loss: 0.02268352173268795
(54478.39) Training batch 1500/7151 loss: 0.07031458616256714
(54607.78) Training batch 2000/7151 loss: 0.0714946985244751
(54737.22) Training batch 2500/7151 loss: 0.6645174026489258
(54866.62) Training batch 3000/7151 loss: 0.027974415570497513
(54995.87) Training batch 3500/7151 loss: 0.019888846203684807
(55125.46) Training batch 4000/7151 loss: 0.17860448360443115
(55254.53) Training batch 4500/7151 loss: 0.00027498145936988294
(55383.23) Training batch 5000/7151 loss: 0.0028788635972887278
(55511.82) Training batch 5500/7151 loss: 0.013950411230325699
(55640.58) Training batch 6000/7151 loss: 0.03689028322696686
(55769.19) Training batch 6500/7151 loss: 0.149245023727417
(55898.12) Training batch 7000/7151 loss: 0.05965817719697952
(55936.72) Epoch 26 on training dataset: loss: 4508.7348775866085; positive-loss: 3980.38; negative-loss: 528.36 
(55936.72)------------- Training epoch 27 started -------------
(56168.54) Epoch 27 on test dataset: MRR: 0.46; Recalls: tensor([0.3346, 0.5918, 0.6966]) Loss: 49048.64positive-loss: 211.28; negative-loss: 48837.35 
(56168.80) Training batch 0/7151 loss: 0.03494482487440109
(56297.39) Training batch 500/7151 loss: 0.016196440905332565
(56426.30) Training batch 1000/7151 loss: 0.01442316360771656
(56555.19) Training batch 1500/7151 loss: 0.0015453672967851162
(56683.97) Training batch 2000/7151 loss: 0.008698908612132072
(56812.57) Training batch 2500/7151 loss: 0.28489887714385986
(56941.41) Training batch 3000/7151 loss: 0.009102319367229939
(57070.26) Training batch 3500/7151 loss: 0.08878686279058456
(57198.89) Training batch 4000/7151 loss: 0.025889212265610695
(57328.00) Training batch 4500/7151 loss: 0.00024798413505777717
(57457.09) Training batch 5000/7151 loss: 0.006887173745781183
(57586.24) Training batch 5500/7151 loss: 0.005408136174082756
(57715.02) Training batch 6000/7151 loss: 0.0021977799478918314
(57844.02) Training batch 6500/7151 loss: 0.039149608463048935
(57973.01) Training batch 7000/7151 loss: 0.05829699710011482
(58011.42) Epoch 27 on training dataset: loss: 4125.551236524213; positive-loss: 3641.30; negative-loss: 484.25 
(58011.42)------------- Training epoch 28 started -------------
(58242.96) Epoch 28 on test dataset: MRR: 0.45; Recalls: tensor([0.3255, 0.5723, 0.6491]) Loss: 56033.75positive-loss: 193.98; negative-loss: 55839.77 
(58243.23) Training batch 0/7151 loss: 0.06188897043466568
(58371.96) Training batch 500/7151 loss: 0.0139089310541749
(58501.12) Training batch 1000/7151 loss: 0.006065926514565945
(58629.80) Training batch 1500/7151 loss: 0.05989690497517586
(58758.77) Training batch 2000/7151 loss: 0.03587618097662926
(58887.66) Training batch 2500/7151 loss: 0.2665488123893738
(59016.55) Training batch 3000/7151 loss: 0.019583825021982193
(59145.46) Training batch 3500/7151 loss: 0.026787467300891876
(59274.25) Training batch 4000/7151 loss: 0.27264732122421265
(59403.15) Training batch 4500/7151 loss: 0.00044850172707811
(59531.87) Training batch 5000/7151 loss: 0.00430840440094471
(59660.69) Training batch 5500/7151 loss: 0.04748846963047981
(59789.85) Training batch 6000/7151 loss: 0.02654043212532997
(59918.65) Training batch 6500/7151 loss: 0.2505156397819519
(60047.35) Training batch 7000/7151 loss: 0.006867459509521723
(60085.77) Epoch 28 on training dataset: loss: 4160.797889989307; positive-loss: 3676.78; negative-loss: 484.01 
(60085.77)------------- Training epoch 29 started -------------
(60317.49) Epoch 29 on test dataset: MRR: 0.46; Recalls: tensor([0.3385, 0.6061, 0.6901]) Loss: 57363.28positive-loss: 190.82; negative-loss: 57172.46 
(60317.76) Training batch 0/7151 loss: 0.10737328976392746
(60446.57) Training batch 500/7151 loss: 0.0001468504488002509
(60575.76) Training batch 1000/7151 loss: 0.0328657440841198
(60704.66) Training batch 1500/7151 loss: 0.21474096179008484
(60833.57) Training batch 2000/7151 loss: 0.04016612097620964
(60962.46) Training batch 2500/7151 loss: 0.10451585799455643
(61091.07) Training batch 3000/7151 loss: 0.06592696905136108
(61220.11) Training batch 3500/7151 loss: 0.0014052612241357565
(61348.91) Training batch 4000/7151 loss: 0.056011781096458435
(61477.57) Training batch 4500/7151 loss: 0.006465558893978596
(61606.54) Training batch 5000/7151 loss: 0.017015889286994934
(61735.10) Training batch 5500/7151 loss: 0.1881302297115326
(61863.71) Training batch 6000/7151 loss: 0.011447648517787457
(61992.31) Training batch 6500/7151 loss: 0.2346118539571762
(62123.40) Training batch 7000/7151 loss: 0.018001290038228035
(62161.85) Epoch 29 on training dataset: loss: 4017.6818753647567; positive-loss: 3539.87; negative-loss: 477.81 
Model saved to ce_lr1e-5_wd1e-1_dr0.2.pt
(63479.65) Final test on test dataset!MRR: 0.42; Recalls: tensor([0.3184, 0.5260, 0.5931]) Loss: 70937.09positive-loss: 33.35; negative-loss: 70903.74 
